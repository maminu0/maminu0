{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maminu0/maminu0/blob/main/teacher_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjtyd1tFkV9M",
        "outputId": "9bd65651-ec53-4038-d537-dbf6562033da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: read data from my google drive and list items and paths of item in the drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "# root_dir = \"/content/drive/My Drive/\"\n",
        "# for dir_name, _, file_names in os.walk(root_dir):\n",
        "#   for file_name in file_names:\n",
        "#     print(os.path.join(dir_name, file_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYJ03jelnUNo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWxAGhLcld84"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.compat.v1 import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape, GlobalMaxPool2D, GRU\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "#import numpy as np\n",
        "import warnings, time\n",
        "warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy2KafD-ltng"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/mmd_dataset.csv')\n",
        "data.head()\n",
        "\n",
        "# GLOBAL VARIABLES\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\n",
        "styles=['-',':','-.','--','-',':','-.','--','-',':','-.','--','-']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NH9wuPrR83t5",
        "outputId": "41f5d80f-2ef5-4b34-8f43-b07e66d7f7fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4\n",
              "1    9\n",
              "2    6\n",
              "3    4\n",
              "4    5\n",
              "Name: Class, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataX = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "ytrain = data[\"Class\"]\n",
        "styles=['-=-',':','-.','-.-','-',':','-.','--','-',':','-.','--','-']\n",
        "ytrain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "YJoPKyv28-0L",
        "outputId": "843ef2dd-d35a-441d-e065-f1149f279b99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0     1     2     3     4     5     6     7     8     9  ...  :dword  \\\n",
              "0   3552   439   437   473   538   426   419   420   402   392  ...     747   \n",
              "1  94955  1057   591   658   554   723   455   470   654   370  ...    3581   \n",
              "2   9940  1167   666   700   560   582   644   639   521   511  ...     366   \n",
              "3   4130   366   418   494   501   417   416   405   402   359  ...     825   \n",
              "4   9838  1807  1507  1276  1822  1346  1240  1033  1494  1216  ...     675   \n",
              "\n",
              "   edx   esi  eax  ebx  ecx  edi  ebp  esp  eip  \n",
              "0  627   496  476  167  285    0    1    0   10  \n",
              "1  579  1872  815  538  883    0   30   12  498  \n",
              "2  133   215  134  109  318    0   23    0   51  \n",
              "3  600   675  643  145  296    0    3    0   20  \n",
              "4  273   243  220  379   61    0    9    0   73  \n",
              "\n",
              "[5 rows x 305 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d1571e4-7b7c-4f79-8645-2b2aa354d99f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>:dword</th>\n",
              "      <th>edx</th>\n",
              "      <th>esi</th>\n",
              "      <th>eax</th>\n",
              "      <th>ebx</th>\n",
              "      <th>ecx</th>\n",
              "      <th>edi</th>\n",
              "      <th>ebp</th>\n",
              "      <th>esp</th>\n",
              "      <th>eip</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3552</td>\n",
              "      <td>439</td>\n",
              "      <td>437</td>\n",
              "      <td>473</td>\n",
              "      <td>538</td>\n",
              "      <td>426</td>\n",
              "      <td>419</td>\n",
              "      <td>420</td>\n",
              "      <td>402</td>\n",
              "      <td>392</td>\n",
              "      <td>...</td>\n",
              "      <td>747</td>\n",
              "      <td>627</td>\n",
              "      <td>496</td>\n",
              "      <td>476</td>\n",
              "      <td>167</td>\n",
              "      <td>285</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>94955</td>\n",
              "      <td>1057</td>\n",
              "      <td>591</td>\n",
              "      <td>658</td>\n",
              "      <td>554</td>\n",
              "      <td>723</td>\n",
              "      <td>455</td>\n",
              "      <td>470</td>\n",
              "      <td>654</td>\n",
              "      <td>370</td>\n",
              "      <td>...</td>\n",
              "      <td>3581</td>\n",
              "      <td>579</td>\n",
              "      <td>1872</td>\n",
              "      <td>815</td>\n",
              "      <td>538</td>\n",
              "      <td>883</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>12</td>\n",
              "      <td>498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9940</td>\n",
              "      <td>1167</td>\n",
              "      <td>666</td>\n",
              "      <td>700</td>\n",
              "      <td>560</td>\n",
              "      <td>582</td>\n",
              "      <td>644</td>\n",
              "      <td>639</td>\n",
              "      <td>521</td>\n",
              "      <td>511</td>\n",
              "      <td>...</td>\n",
              "      <td>366</td>\n",
              "      <td>133</td>\n",
              "      <td>215</td>\n",
              "      <td>134</td>\n",
              "      <td>109</td>\n",
              "      <td>318</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4130</td>\n",
              "      <td>366</td>\n",
              "      <td>418</td>\n",
              "      <td>494</td>\n",
              "      <td>501</td>\n",
              "      <td>417</td>\n",
              "      <td>416</td>\n",
              "      <td>405</td>\n",
              "      <td>402</td>\n",
              "      <td>359</td>\n",
              "      <td>...</td>\n",
              "      <td>825</td>\n",
              "      <td>600</td>\n",
              "      <td>675</td>\n",
              "      <td>643</td>\n",
              "      <td>145</td>\n",
              "      <td>296</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9838</td>\n",
              "      <td>1807</td>\n",
              "      <td>1507</td>\n",
              "      <td>1276</td>\n",
              "      <td>1822</td>\n",
              "      <td>1346</td>\n",
              "      <td>1240</td>\n",
              "      <td>1033</td>\n",
              "      <td>1494</td>\n",
              "      <td>1216</td>\n",
              "      <td>...</td>\n",
              "      <td>675</td>\n",
              "      <td>273</td>\n",
              "      <td>243</td>\n",
              "      <td>220</td>\n",
              "      <td>379</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 305 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d1571e4-7b7c-4f79-8645-2b2aa354d99f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d1571e4-7b7c-4f79-8645-2b2aa354d99f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d1571e4-7b7c-4f79-8645-2b2aa354d99f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ae3702d1-453a-4860-baf1-2d56a7392c6c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae3702d1-453a-4860-baf1-2d56a7392c6c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ae3702d1-453a-4860-baf1-2d56a7392c6c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataX"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "features = ['0','1','2','3','4','5','6','7','8','9','0a','0b','0c','0d','0e','0f','10','11','12','13','14','15','16','17','18','19',\n",
        "            '1a','1b','1c','1d','1e','1f','20','21','22','23','24','25','26','27','28','29','2a','2b','2c','2d','2e','2f','30','31',\n",
        "            '32','33','34','35','36','37','38','39','3a','3b','3c','3d','3e','3f','40','41','42','43','44','45','46','47','48','49',\n",
        "            '4a','4b','4c','4d','4e','4f','50','51','52','53','54','55','56','57','58','59','5a','5b','5c','5d','5e','5f','60','61',\n",
        "            '62','63','64','65','66','67','68','69','6a','6b','6c','6d','6e','6f','70','71','72','73','74','75','76','77','78','79',\n",
        "            '7a','7b','7c','7d','7e','7f','80','81','82','83','84','85','86','87','88','89','8a','8b','8c','8d','8e','8f','90','91',\n",
        "            '92','93','94','95','96','97','98','99','9a','9b','9c','9d','9e','9f','a0','a1','a2','a3','a4','a5','a6','a7','a8','a9',\n",
        "            'aa','ab','ac','ad','ae','af','b0','b1','b2','b3','b4','b5','b6','b7','b8','b9','ba','bb','bc','bd','be','bf','c0','c1',\n",
        "            'c2','c3','c4','c5','c6','c7','c8','c9','ca','cb','cc','cd','ce','cf','d0','d1','d2','d3','d4','d5','d6','d7','d8','d9',\n",
        "            'da','db','dc','dd','de','df','e0','e1','e2','e3','e4','e5','e6','e7','e8','e9','ea','eb','ec','ed','ee','ef','f0','f1',\n",
        "            'f2','f3','f4','f5','f6','f7','f8','f9',\"fa\",\"fb\",\"fc\",\"fd\",\"fe\",\"ff\",\"??\", 'HEADER:', '.text:', '.Pav:', '.idata:',\n",
        "                '.data:', '.bss:', '.rdata:', '.edata:', '.rsrc:', '.tls:', '.reloc:', 'jmp', 'mov', 'retf', 'push', 'pop', 'xor',\n",
        "            'retn', 'nop', 'sub', 'inc', 'dec', 'add', 'imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb', 'jz',\n",
        "            'lea', 'movzx', '.dll', 'std::', ':dword', 'edx', 'esi', 'eax', 'ebx', 'ecx', 'edi', 'ebp', 'esp', 'eip']\n",
        "dataX=data[features]\n",
        "dataX.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfgpakzBMu06"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IhAG7-u9u9i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDybvkAd9KQ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(ytrain)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "# GLOBAL VARIABLES\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVx4vlLmAS8-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, GlobalAvgPool2D, Reshape, Bidirectional, LSTM, Dense\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bFSVjSQNOtv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "# Define callbacks list\n",
        "callbacks = [annealer]#callbacks = [early_stopping, annealer]#\n",
        "X_train, X_val, y_train_, y_val = train_test_split(dataX, y_train, test_size = 0.2, stratify=y_train, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFY_xB97OPEV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_(hist, name=\"teacher_pattern_plot\"):\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = hist.history['accuracy']\n",
        "    train_loss = hist.history['loss']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    fig.set_size_inches(16,9)\n",
        "\n",
        "    ax[0].plot( train_acc , 'go-' , label = 'Training Accuracy')\n",
        "    ax[0].plot( val_acc , 'ro-' , label = 'Testing Accuracy')\n",
        "    ax[0].set_title('Training & Validation Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "    ax[1].plot( train_loss , 'g-o' , label = 'Training Loss')\n",
        "    ax[1].plot( val_loss , 'r-o' , label = 'Testing Loss')\n",
        "    ax[1].set_title('Testing Accuracy & Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "    plt.savefig(f\"{'_'.join(name)}_{str(time.time())}_{name}_.png\")\n",
        "    plt.show()\n",
        "\n",
        "def pre_cl(model,X_val2,Y_val2, name=\"teacher\"):\n",
        "    predictions = model.predict(X_val2)\n",
        "    Xpred = np.argmax(predictions, axis=1)\n",
        "    ytrue = np.argmax(Y_val2, axis=1)\n",
        "    print(classification_report(ytrue, Xpred))\n",
        "    cm = confusion_matrix(ytrue,Xpred)\n",
        "    sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')\n",
        "    plt.figure(figsize = (15,15))\n",
        "    plt.savefig(f\"{'_'.join(name)}_{str(time.time())}_{name}_.png\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(model,X_test,Y_test, name=\"final\"):\n",
        "\n",
        "    # Assuming you have predictions from your model\n",
        "    y_pred = model.predict(X_test)  # Replace X_test with your test data\n",
        "\n",
        "    # Assuming your labels are one-hot encoded\n",
        "    y_true = np.argmax(Y_test, axis=1)  # Get the class labels from one-hot encoded Y_test\n",
        "\n",
        "    # Calculate the ROC curve and AUC for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes = Y_test.shape[1]\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], y_pred[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Plot the ROC curve for each class\n",
        "    plt.figure()\n",
        "    for i in range(n_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                  .format(i, roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    #plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(f\"{'_'.join(name)}_{str(time.time())}_{name}_.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def inception_module(input_tensor, n_filters=128):\n",
        "    # Bottleneck layer\n",
        "    bottleneck = layers.Conv1D(n_filters, kernel_size=1, kernel_regularizer=tf.keras.regularizers.l2(0.001), padding='same')(input_tensor)\n",
        "    bottleneck = layers.BatchNormalization()(bottleneck)\n",
        "    bottleneck = layers.Activation('relu')(bottleneck)\n",
        "    bottleneck = layers.Dropout(0.1)(bottleneck)\n",
        "\n",
        "    # Inception layers\n",
        "    conv1 = layers.Conv1D(n_filters, kernel_size=1, padding='same')(bottleneck)\n",
        "    conv3 = layers.Conv1D(n_filters, kernel_size=3, padding='same')(bottleneck)\n",
        "    conv5 = layers.Conv1D(n_filters, kernel_size=5, padding='same')(bottleneck)\n",
        "    maxpool = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(bottleneck)\n",
        "    maxpool = layers.Conv1D(n_filters, kernel_size=1, padding='same')(maxpool)\n",
        "\n",
        "    # Concatenate\n",
        "    output = layers.concatenate([conv1, conv3, conv5, maxpool], axis=-1)\n",
        "    return output\n",
        "\n",
        "def InceptionTime(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    for _ in range(n_modules):\n",
        "        x = inception_module(x, n_filters=n_filters)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "Inceptionmodel = InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "Inceptionmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "Inceptionmodel.summary()\n",
        "Inceptionmodel_history = Inceptionmodel.fit(X_train, y_train_, epochs=200, batch_size=32, verbose=2, validation_data=(X_val, y_val), callbacks=[annealer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CvDyDSPneDIP",
        "outputId": "0b3a7674-035e-4143-f02d-a6fa622258fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,617</span> │ global_average_poolin… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_16 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_18 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_19 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_21 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_22 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_23 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_25 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_26 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_27 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m49,280\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_28 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m82,048\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_29 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │          \u001b[38;5;34m4,617\u001b[0m │ global_average_poolin… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,322,377</span> (5.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,322,377\u001b[0m (5.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,320,841</span> (5.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,320,841\u001b[0m (5.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "272/272 - 52s - 193ms/step - accuracy: 0.6920 - loss: 1.5040 - val_accuracy: 0.2861 - val_loss: 5.8164 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.7557 - loss: 0.9919 - val_accuracy: 0.3634 - val_loss: 5.9594 - learning_rate: 9.5000e-04\n",
            "Epoch 3/200\n",
            "272/272 - 20s - 74ms/step - accuracy: 0.8009 - loss: 0.7748 - val_accuracy: 0.4259 - val_loss: 3.2169 - learning_rate: 9.0250e-04\n",
            "Epoch 4/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.8164 - loss: 0.7142 - val_accuracy: 0.2755 - val_loss: 15.2195 - learning_rate: 8.5737e-04\n",
            "Epoch 5/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.8003 - loss: 0.7471 - val_accuracy: 0.6265 - val_loss: 1.2698 - learning_rate: 8.1451e-04\n",
            "Epoch 6/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.8305 - loss: 0.6361 - val_accuracy: 0.4503 - val_loss: 2.0163 - learning_rate: 7.7378e-04\n",
            "Epoch 7/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.8481 - loss: 0.5675 - val_accuracy: 0.5414 - val_loss: 1.6905 - learning_rate: 7.3509e-04\n",
            "Epoch 8/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.8298 - loss: 0.6326 - val_accuracy: 0.2282 - val_loss: 3.8922 - learning_rate: 6.9834e-04\n",
            "Epoch 9/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.8637 - loss: 0.5290 - val_accuracy: 0.5000 - val_loss: 2.0762 - learning_rate: 6.6342e-04\n",
            "Epoch 10/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.8758 - loss: 0.4920 - val_accuracy: 0.5823 - val_loss: 2.0102 - learning_rate: 6.3025e-04\n",
            "Epoch 11/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.8814 - loss: 0.4613 - val_accuracy: 0.5796 - val_loss: 1.3303 - learning_rate: 5.9874e-04\n",
            "Epoch 12/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.8713 - loss: 0.4993 - val_accuracy: 0.1601 - val_loss: 8.4767 - learning_rate: 5.6880e-04\n",
            "Epoch 13/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.8666 - loss: 0.5327 - val_accuracy: 0.5989 - val_loss: 1.6088 - learning_rate: 5.4036e-04\n",
            "Epoch 14/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.8822 - loss: 0.4627 - val_accuracy: 0.7148 - val_loss: 1.5941 - learning_rate: 5.1334e-04\n",
            "Epoch 15/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9026 - loss: 0.4065 - val_accuracy: 0.7139 - val_loss: 1.4992 - learning_rate: 4.8767e-04\n",
            "Epoch 16/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9020 - loss: 0.3969 - val_accuracy: 0.5736 - val_loss: 1.9860 - learning_rate: 4.6329e-04\n",
            "Epoch 17/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9092 - loss: 0.3806 - val_accuracy: 0.7328 - val_loss: 1.3106 - learning_rate: 4.4013e-04\n",
            "Epoch 18/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9176 - loss: 0.3640 - val_accuracy: 0.5920 - val_loss: 2.1224 - learning_rate: 4.1812e-04\n",
            "Epoch 19/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9195 - loss: 0.3566 - val_accuracy: 0.6624 - val_loss: 1.7663 - learning_rate: 3.9721e-04\n",
            "Epoch 20/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9259 - loss: 0.3357 - val_accuracy: 0.7185 - val_loss: 1.3787 - learning_rate: 3.7735e-04\n",
            "Epoch 21/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9289 - loss: 0.3219 - val_accuracy: 0.7259 - val_loss: 1.5104 - learning_rate: 3.5849e-04\n",
            "Epoch 22/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9202 - loss: 0.3384 - val_accuracy: 0.7856 - val_loss: 0.8987 - learning_rate: 3.4056e-04\n",
            "Epoch 23/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9336 - loss: 0.3138 - val_accuracy: 0.7286 - val_loss: 1.2967 - learning_rate: 3.2353e-04\n",
            "Epoch 24/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9393 - loss: 0.2861 - val_accuracy: 0.7208 - val_loss: 1.2062 - learning_rate: 3.0736e-04\n",
            "Epoch 25/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9425 - loss: 0.2698 - val_accuracy: 0.7576 - val_loss: 1.2372 - learning_rate: 2.9199e-04\n",
            "Epoch 26/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9412 - loss: 0.2739 - val_accuracy: 0.7466 - val_loss: 0.9758 - learning_rate: 2.7739e-04\n",
            "Epoch 27/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9425 - loss: 0.2596 - val_accuracy: 0.6440 - val_loss: 2.0587 - learning_rate: 2.6352e-04\n",
            "Epoch 28/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9499 - loss: 0.2532 - val_accuracy: 0.8192 - val_loss: 0.7158 - learning_rate: 2.5034e-04\n",
            "Epoch 29/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9493 - loss: 0.2341 - val_accuracy: 0.6302 - val_loss: 1.7339 - learning_rate: 2.3783e-04\n",
            "Epoch 30/200\n",
            "272/272 - 21s - 78ms/step - accuracy: 0.9467 - loss: 0.2449 - val_accuracy: 0.7686 - val_loss: 1.1345 - learning_rate: 2.2594e-04\n",
            "Epoch 31/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9515 - loss: 0.2320 - val_accuracy: 0.7755 - val_loss: 1.4580 - learning_rate: 2.1464e-04\n",
            "Epoch 32/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9533 - loss: 0.2186 - val_accuracy: 0.7921 - val_loss: 1.0322 - learning_rate: 2.0391e-04\n",
            "Epoch 33/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9573 - loss: 0.2065 - val_accuracy: 0.7921 - val_loss: 0.9261 - learning_rate: 1.9371e-04\n",
            "Epoch 34/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9526 - loss: 0.2209 - val_accuracy: 0.8013 - val_loss: 0.9147 - learning_rate: 1.8403e-04\n",
            "Epoch 35/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9574 - loss: 0.2002 - val_accuracy: 0.8266 - val_loss: 0.4842 - learning_rate: 1.7482e-04\n",
            "Epoch 36/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9574 - loss: 0.1919 - val_accuracy: 0.8077 - val_loss: 0.7518 - learning_rate: 1.6608e-04\n",
            "Epoch 37/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9573 - loss: 0.1899 - val_accuracy: 0.7709 - val_loss: 0.9953 - learning_rate: 1.5778e-04\n",
            "Epoch 38/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9630 - loss: 0.1837 - val_accuracy: 0.9586 - val_loss: 0.2291 - learning_rate: 1.4989e-04\n",
            "Epoch 39/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9617 - loss: 0.1817 - val_accuracy: 0.7677 - val_loss: 1.2449 - learning_rate: 1.4240e-04\n",
            "Epoch 40/200\n",
            "272/272 - 21s - 78ms/step - accuracy: 0.9632 - loss: 0.1744 - val_accuracy: 0.8477 - val_loss: 0.7810 - learning_rate: 1.3528e-04\n",
            "Epoch 41/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9609 - loss: 0.1796 - val_accuracy: 0.8234 - val_loss: 0.7636 - learning_rate: 1.2851e-04\n",
            "Epoch 42/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9662 - loss: 0.1654 - val_accuracy: 0.8772 - val_loss: 0.5940 - learning_rate: 1.2209e-04\n",
            "Epoch 43/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9618 - loss: 0.1856 - val_accuracy: 0.9154 - val_loss: 0.4993 - learning_rate: 1.1598e-04\n",
            "Epoch 44/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9643 - loss: 0.1666 - val_accuracy: 0.9080 - val_loss: 0.4998 - learning_rate: 1.1018e-04\n",
            "Epoch 45/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9641 - loss: 0.1644 - val_accuracy: 0.9062 - val_loss: 0.3462 - learning_rate: 1.0467e-04\n",
            "Epoch 46/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9670 - loss: 0.1619 - val_accuracy: 0.8013 - val_loss: 0.9626 - learning_rate: 9.9440e-05\n",
            "Epoch 47/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9649 - loss: 0.1594 - val_accuracy: 0.8841 - val_loss: 0.7741 - learning_rate: 9.4468e-05\n",
            "Epoch 48/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9695 - loss: 0.1492 - val_accuracy: 0.8114 - val_loss: 0.8484 - learning_rate: 8.9745e-05\n",
            "Epoch 49/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9704 - loss: 0.1447 - val_accuracy: 0.8937 - val_loss: 0.5939 - learning_rate: 8.5258e-05\n",
            "Epoch 50/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9699 - loss: 0.1454 - val_accuracy: 0.9029 - val_loss: 0.5434 - learning_rate: 8.0995e-05\n",
            "Epoch 51/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9689 - loss: 0.1453 - val_accuracy: 0.8615 - val_loss: 0.7810 - learning_rate: 7.6945e-05\n",
            "Epoch 52/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9693 - loss: 0.1465 - val_accuracy: 0.9282 - val_loss: 0.3436 - learning_rate: 7.3098e-05\n",
            "Epoch 53/200\n",
            "272/272 - 20s - 74ms/step - accuracy: 0.9714 - loss: 0.1364 - val_accuracy: 0.9002 - val_loss: 0.6261 - learning_rate: 6.9443e-05\n",
            "Epoch 54/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9731 - loss: 0.1347 - val_accuracy: 0.8220 - val_loss: 0.6554 - learning_rate: 6.5971e-05\n",
            "Epoch 55/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9720 - loss: 0.1354 - val_accuracy: 0.9131 - val_loss: 0.5291 - learning_rate: 6.2672e-05\n",
            "Epoch 56/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9723 - loss: 0.1330 - val_accuracy: 0.9131 - val_loss: 0.5575 - learning_rate: 5.9539e-05\n",
            "Epoch 57/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9727 - loss: 0.1307 - val_accuracy: 0.9006 - val_loss: 0.4590 - learning_rate: 5.6562e-05\n",
            "Epoch 58/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9680 - loss: 0.1356 - val_accuracy: 0.9066 - val_loss: 0.5207 - learning_rate: 5.3734e-05\n",
            "Epoch 59/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9731 - loss: 0.1275 - val_accuracy: 0.9075 - val_loss: 0.6420 - learning_rate: 5.1047e-05\n",
            "Epoch 60/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9752 - loss: 0.1226 - val_accuracy: 0.8565 - val_loss: 0.6111 - learning_rate: 4.8495e-05\n",
            "Epoch 61/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9746 - loss: 0.1271 - val_accuracy: 0.9144 - val_loss: 0.4469 - learning_rate: 4.6070e-05\n",
            "Epoch 62/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9741 - loss: 0.1188 - val_accuracy: 0.9393 - val_loss: 0.2548 - learning_rate: 4.3766e-05\n",
            "Epoch 63/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9769 - loss: 0.1166 - val_accuracy: 0.9103 - val_loss: 0.3654 - learning_rate: 4.1578e-05\n",
            "Epoch 64/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9739 - loss: 0.1229 - val_accuracy: 0.9264 - val_loss: 0.4244 - learning_rate: 3.9499e-05\n",
            "Epoch 65/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9741 - loss: 0.1184 - val_accuracy: 0.9232 - val_loss: 0.3130 - learning_rate: 3.7524e-05\n",
            "Epoch 66/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9750 - loss: 0.1163 - val_accuracy: 0.9223 - val_loss: 0.4017 - learning_rate: 3.5648e-05\n",
            "Epoch 67/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9768 - loss: 0.1121 - val_accuracy: 0.9618 - val_loss: 0.1984 - learning_rate: 3.3866e-05\n",
            "Epoch 68/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9760 - loss: 0.1180 - val_accuracy: 0.9223 - val_loss: 0.3340 - learning_rate: 3.2172e-05\n",
            "Epoch 69/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9769 - loss: 0.1118 - val_accuracy: 0.9154 - val_loss: 0.4489 - learning_rate: 3.0564e-05\n",
            "Epoch 70/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9768 - loss: 0.1109 - val_accuracy: 0.9181 - val_loss: 0.4290 - learning_rate: 2.9035e-05\n",
            "Epoch 71/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9771 - loss: 0.1158 - val_accuracy: 0.9126 - val_loss: 0.4308 - learning_rate: 2.7584e-05\n",
            "Epoch 72/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9767 - loss: 0.1119 - val_accuracy: 0.9315 - val_loss: 0.2728 - learning_rate: 2.6205e-05\n",
            "Epoch 73/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9778 - loss: 0.1087 - val_accuracy: 0.9278 - val_loss: 0.3680 - learning_rate: 2.4894e-05\n",
            "Epoch 74/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9767 - loss: 0.1092 - val_accuracy: 0.9232 - val_loss: 0.3795 - learning_rate: 2.3650e-05\n",
            "Epoch 75/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9777 - loss: 0.1116 - val_accuracy: 0.9287 - val_loss: 0.2887 - learning_rate: 2.2467e-05\n",
            "Epoch 76/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9765 - loss: 0.1103 - val_accuracy: 0.9282 - val_loss: 0.2966 - learning_rate: 2.1344e-05\n",
            "Epoch 77/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9770 - loss: 0.1053 - val_accuracy: 0.9315 - val_loss: 0.2945 - learning_rate: 2.0277e-05\n",
            "Epoch 78/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9800 - loss: 0.0992 - val_accuracy: 0.9333 - val_loss: 0.2905 - learning_rate: 1.9263e-05\n",
            "Epoch 79/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9795 - loss: 0.1040 - val_accuracy: 0.9264 - val_loss: 0.2908 - learning_rate: 1.8300e-05\n",
            "Epoch 80/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9779 - loss: 0.1020 - val_accuracy: 0.9264 - val_loss: 0.3255 - learning_rate: 1.7385e-05\n",
            "Epoch 81/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9792 - loss: 0.1037 - val_accuracy: 0.9209 - val_loss: 0.4204 - learning_rate: 1.6515e-05\n",
            "Epoch 82/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9798 - loss: 0.0993 - val_accuracy: 0.9328 - val_loss: 0.3138 - learning_rate: 1.5690e-05\n",
            "Epoch 83/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9785 - loss: 0.1057 - val_accuracy: 0.9250 - val_loss: 0.3052 - learning_rate: 1.4905e-05\n",
            "Epoch 84/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9784 - loss: 0.1039 - val_accuracy: 0.9255 - val_loss: 0.3143 - learning_rate: 1.4160e-05\n",
            "Epoch 85/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9783 - loss: 0.1010 - val_accuracy: 0.9218 - val_loss: 0.3619 - learning_rate: 1.3452e-05\n",
            "Epoch 86/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9779 - loss: 0.1010 - val_accuracy: 0.9328 - val_loss: 0.2672 - learning_rate: 1.2779e-05\n",
            "Epoch 87/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9806 - loss: 0.0949 - val_accuracy: 0.9338 - val_loss: 0.2828 - learning_rate: 1.2140e-05\n",
            "Epoch 88/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9806 - loss: 0.0969 - val_accuracy: 0.9324 - val_loss: 0.2806 - learning_rate: 1.1533e-05\n",
            "Epoch 89/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9791 - loss: 0.0972 - val_accuracy: 0.9310 - val_loss: 0.2849 - learning_rate: 1.0957e-05\n",
            "Epoch 90/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9804 - loss: 0.1008 - val_accuracy: 0.9232 - val_loss: 0.3829 - learning_rate: 1.0409e-05\n",
            "Epoch 91/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9799 - loss: 0.0979 - val_accuracy: 0.9319 - val_loss: 0.2795 - learning_rate: 9.8884e-06\n",
            "Epoch 92/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9815 - loss: 0.0974 - val_accuracy: 0.9250 - val_loss: 0.3636 - learning_rate: 9.3939e-06\n",
            "Epoch 93/200\n",
            "272/272 - 20s - 72ms/step - accuracy: 0.9807 - loss: 0.0947 - val_accuracy: 0.9310 - val_loss: 0.2296 - learning_rate: 8.9242e-06\n",
            "Epoch 94/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9794 - loss: 0.0961 - val_accuracy: 0.9241 - val_loss: 0.3348 - learning_rate: 8.4780e-06\n",
            "Epoch 95/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9794 - loss: 0.1002 - val_accuracy: 0.9273 - val_loss: 0.2828 - learning_rate: 8.0541e-06\n",
            "Epoch 96/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9813 - loss: 0.0977 - val_accuracy: 0.9305 - val_loss: 0.2361 - learning_rate: 7.6514e-06\n",
            "Epoch 97/200\n",
            "272/272 - 10s - 39ms/step - accuracy: 0.9786 - loss: 0.1037 - val_accuracy: 0.9646 - val_loss: 0.1793 - learning_rate: 7.2689e-06\n",
            "Epoch 98/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9787 - loss: 0.1007 - val_accuracy: 0.9641 - val_loss: 0.1938 - learning_rate: 6.9054e-06\n",
            "Epoch 99/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9810 - loss: 0.0912 - val_accuracy: 0.9411 - val_loss: 0.2137 - learning_rate: 6.5601e-06\n",
            "Epoch 100/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9809 - loss: 0.0934 - val_accuracy: 0.9292 - val_loss: 0.2805 - learning_rate: 6.2321e-06\n",
            "Epoch 101/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9804 - loss: 0.0951 - val_accuracy: 0.9292 - val_loss: 0.2935 - learning_rate: 5.9205e-06\n",
            "Epoch 102/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9792 - loss: 0.0985 - val_accuracy: 0.9278 - val_loss: 0.2939 - learning_rate: 5.6245e-06\n",
            "Epoch 103/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9810 - loss: 0.0956 - val_accuracy: 0.9301 - val_loss: 0.2695 - learning_rate: 5.3433e-06\n",
            "Epoch 104/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9787 - loss: 0.0975 - val_accuracy: 0.9324 - val_loss: 0.2683 - learning_rate: 5.0761e-06\n",
            "Epoch 105/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9795 - loss: 0.0935 - val_accuracy: 0.9218 - val_loss: 0.4710 - learning_rate: 4.8223e-06\n",
            "Epoch 106/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9802 - loss: 0.0930 - val_accuracy: 0.9278 - val_loss: 0.2890 - learning_rate: 4.5812e-06\n",
            "Epoch 107/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9804 - loss: 0.0966 - val_accuracy: 0.9315 - val_loss: 0.2631 - learning_rate: 4.3521e-06\n",
            "Epoch 108/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9808 - loss: 0.0902 - val_accuracy: 0.9236 - val_loss: 0.3485 - learning_rate: 4.1345e-06\n",
            "Epoch 109/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9791 - loss: 0.0910 - val_accuracy: 0.9301 - val_loss: 0.2464 - learning_rate: 3.9278e-06\n",
            "Epoch 110/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9817 - loss: 0.0901 - val_accuracy: 0.9264 - val_loss: 0.2989 - learning_rate: 3.7314e-06\n",
            "Epoch 111/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9802 - loss: 0.0964 - val_accuracy: 0.9200 - val_loss: 0.4368 - learning_rate: 3.5448e-06\n",
            "Epoch 112/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9800 - loss: 0.0931 - val_accuracy: 0.9310 - val_loss: 0.2500 - learning_rate: 3.3676e-06\n",
            "Epoch 113/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9813 - loss: 0.0905 - val_accuracy: 0.9278 - val_loss: 0.3253 - learning_rate: 3.1992e-06\n",
            "Epoch 114/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9814 - loss: 0.0912 - val_accuracy: 0.9301 - val_loss: 0.2491 - learning_rate: 3.0393e-06\n",
            "Epoch 115/200\n",
            "272/272 - 21s - 78ms/step - accuracy: 0.9809 - loss: 0.0908 - val_accuracy: 0.9338 - val_loss: 0.2261 - learning_rate: 2.8873e-06\n",
            "Epoch 116/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9811 - loss: 0.0889 - val_accuracy: 0.9324 - val_loss: 0.2206 - learning_rate: 2.7429e-06\n",
            "Epoch 117/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9815 - loss: 0.0896 - val_accuracy: 0.9319 - val_loss: 0.2294 - learning_rate: 2.6058e-06\n",
            "Epoch 118/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9804 - loss: 0.0945 - val_accuracy: 0.9204 - val_loss: 0.3574 - learning_rate: 2.4755e-06\n",
            "Epoch 119/200\n",
            "272/272 - 20s - 74ms/step - accuracy: 0.9813 - loss: 0.0887 - val_accuracy: 0.9310 - val_loss: 0.2697 - learning_rate: 2.3517e-06\n",
            "Epoch 120/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9807 - loss: 0.0933 - val_accuracy: 0.9324 - val_loss: 0.2475 - learning_rate: 2.2341e-06\n",
            "Epoch 121/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9811 - loss: 0.0912 - val_accuracy: 0.9282 - val_loss: 0.2960 - learning_rate: 2.1224e-06\n",
            "Epoch 122/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9796 - loss: 0.0933 - val_accuracy: 0.9264 - val_loss: 0.3099 - learning_rate: 2.0163e-06\n",
            "Epoch 123/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9817 - loss: 0.0879 - val_accuracy: 0.9292 - val_loss: 0.2772 - learning_rate: 1.9155e-06\n",
            "Epoch 124/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9806 - loss: 0.0930 - val_accuracy: 0.9319 - val_loss: 0.2351 - learning_rate: 1.8197e-06\n",
            "Epoch 125/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9811 - loss: 0.0937 - val_accuracy: 0.9315 - val_loss: 0.2377 - learning_rate: 1.7287e-06\n",
            "Epoch 126/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9814 - loss: 0.0885 - val_accuracy: 0.9319 - val_loss: 0.2208 - learning_rate: 1.6423e-06\n",
            "Epoch 127/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9822 - loss: 0.0872 - val_accuracy: 0.9236 - val_loss: 0.3478 - learning_rate: 1.5602e-06\n",
            "Epoch 128/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9818 - loss: 0.0913 - val_accuracy: 0.9269 - val_loss: 0.3256 - learning_rate: 1.4822e-06\n",
            "Epoch 129/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9815 - loss: 0.0903 - val_accuracy: 0.9448 - val_loss: 0.2049 - learning_rate: 1.4081e-06\n",
            "Epoch 130/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9808 - loss: 0.0898 - val_accuracy: 0.9250 - val_loss: 0.4194 - learning_rate: 1.3377e-06\n",
            "Epoch 131/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9827 - loss: 0.0899 - val_accuracy: 0.9416 - val_loss: 0.2065 - learning_rate: 1.2708e-06\n",
            "Epoch 132/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9793 - loss: 0.0907 - val_accuracy: 0.9356 - val_loss: 0.2086 - learning_rate: 1.2072e-06\n",
            "Epoch 133/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9796 - loss: 0.0918 - val_accuracy: 0.9347 - val_loss: 0.2085 - learning_rate: 1.1469e-06\n",
            "Epoch 134/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9800 - loss: 0.0936 - val_accuracy: 0.9328 - val_loss: 0.2283 - learning_rate: 1.0895e-06\n",
            "Epoch 135/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9793 - loss: 0.0947 - val_accuracy: 0.9310 - val_loss: 0.2949 - learning_rate: 1.0351e-06\n",
            "Epoch 136/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9807 - loss: 0.0910 - val_accuracy: 0.9338 - val_loss: 0.2216 - learning_rate: 9.8330e-07\n",
            "Epoch 137/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9810 - loss: 0.0923 - val_accuracy: 0.9269 - val_loss: 0.3605 - learning_rate: 9.3414e-07\n",
            "Epoch 138/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9811 - loss: 0.0948 - val_accuracy: 0.9315 - val_loss: 0.2735 - learning_rate: 8.8743e-07\n",
            "Epoch 139/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9798 - loss: 0.0927 - val_accuracy: 0.9361 - val_loss: 0.2437 - learning_rate: 8.4306e-07\n",
            "Epoch 140/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9802 - loss: 0.0932 - val_accuracy: 0.9342 - val_loss: 0.2293 - learning_rate: 8.0091e-07\n",
            "Epoch 141/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9791 - loss: 0.0940 - val_accuracy: 0.9259 - val_loss: 0.3794 - learning_rate: 7.6086e-07\n",
            "Epoch 142/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9801 - loss: 0.0928 - val_accuracy: 0.9296 - val_loss: 0.2690 - learning_rate: 7.2282e-07\n",
            "Epoch 143/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9810 - loss: 0.0878 - val_accuracy: 0.9310 - val_loss: 0.2555 - learning_rate: 6.8668e-07\n",
            "Epoch 144/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9808 - loss: 0.0873 - val_accuracy: 0.9310 - val_loss: 0.2699 - learning_rate: 6.5234e-07\n",
            "Epoch 145/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9822 - loss: 0.0868 - val_accuracy: 0.9319 - val_loss: 0.2397 - learning_rate: 6.1973e-07\n",
            "Epoch 146/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9800 - loss: 0.0928 - val_accuracy: 0.9227 - val_loss: 0.3890 - learning_rate: 5.8874e-07\n",
            "Epoch 147/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9813 - loss: 0.0886 - val_accuracy: 0.9310 - val_loss: 0.2664 - learning_rate: 5.5930e-07\n",
            "Epoch 148/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9806 - loss: 0.0941 - val_accuracy: 0.9310 - val_loss: 0.2706 - learning_rate: 5.3134e-07\n",
            "Epoch 149/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9827 - loss: 0.0881 - val_accuracy: 0.9305 - val_loss: 0.2568 - learning_rate: 5.0477e-07\n",
            "Epoch 150/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9809 - loss: 0.0893 - val_accuracy: 0.9328 - val_loss: 0.2391 - learning_rate: 4.7953e-07\n",
            "Epoch 151/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9813 - loss: 0.0876 - val_accuracy: 0.9319 - val_loss: 0.2240 - learning_rate: 4.5555e-07\n",
            "Epoch 152/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9804 - loss: 0.0880 - val_accuracy: 0.9526 - val_loss: 0.2017 - learning_rate: 4.3278e-07\n",
            "Epoch 153/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9836 - loss: 0.0897 - val_accuracy: 0.9319 - val_loss: 0.2397 - learning_rate: 4.1114e-07\n",
            "Epoch 154/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9807 - loss: 0.0921 - val_accuracy: 0.9324 - val_loss: 0.2309 - learning_rate: 3.9058e-07\n",
            "Epoch 155/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9807 - loss: 0.0923 - val_accuracy: 0.9333 - val_loss: 0.2125 - learning_rate: 3.7105e-07\n",
            "Epoch 156/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9803 - loss: 0.0893 - val_accuracy: 0.9324 - val_loss: 0.2292 - learning_rate: 3.5250e-07\n",
            "Epoch 157/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9824 - loss: 0.0904 - val_accuracy: 0.9338 - val_loss: 0.2467 - learning_rate: 3.3487e-07\n",
            "Epoch 158/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9810 - loss: 0.0905 - val_accuracy: 0.9324 - val_loss: 0.2917 - learning_rate: 3.1813e-07\n",
            "Epoch 159/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9819 - loss: 0.0888 - val_accuracy: 0.9591 - val_loss: 0.1926 - learning_rate: 3.0222e-07\n",
            "Epoch 160/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9818 - loss: 0.0884 - val_accuracy: 0.9328 - val_loss: 0.2442 - learning_rate: 2.8711e-07\n",
            "Epoch 161/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9816 - loss: 0.0903 - val_accuracy: 0.9333 - val_loss: 0.2264 - learning_rate: 2.7276e-07\n",
            "Epoch 162/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9814 - loss: 0.0921 - val_accuracy: 0.9213 - val_loss: 0.4459 - learning_rate: 2.5912e-07\n",
            "Epoch 163/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9822 - loss: 0.0919 - val_accuracy: 0.9278 - val_loss: 0.3113 - learning_rate: 2.4616e-07\n",
            "Epoch 164/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9818 - loss: 0.0880 - val_accuracy: 0.9301 - val_loss: 0.2959 - learning_rate: 2.3386e-07\n",
            "Epoch 165/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9813 - loss: 0.0893 - val_accuracy: 0.9319 - val_loss: 0.2448 - learning_rate: 2.2216e-07\n",
            "Epoch 166/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9814 - loss: 0.0925 - val_accuracy: 0.9324 - val_loss: 0.2305 - learning_rate: 2.1105e-07\n",
            "Epoch 167/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9825 - loss: 0.0866 - val_accuracy: 0.9356 - val_loss: 0.2052 - learning_rate: 2.0050e-07\n",
            "Epoch 168/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9803 - loss: 0.0884 - val_accuracy: 0.9365 - val_loss: 0.2102 - learning_rate: 1.9048e-07\n",
            "Epoch 169/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9813 - loss: 0.0910 - val_accuracy: 0.9250 - val_loss: 0.3589 - learning_rate: 1.8095e-07\n",
            "Epoch 170/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9808 - loss: 0.0901 - val_accuracy: 0.9319 - val_loss: 0.2442 - learning_rate: 1.7191e-07\n",
            "Epoch 171/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9806 - loss: 0.0905 - val_accuracy: 0.9328 - val_loss: 0.2153 - learning_rate: 1.6331e-07\n",
            "Epoch 172/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9818 - loss: 0.0894 - val_accuracy: 0.9310 - val_loss: 0.2624 - learning_rate: 1.5514e-07\n",
            "Epoch 173/200\n",
            "272/272 - 20s - 74ms/step - accuracy: 0.9809 - loss: 0.0887 - val_accuracy: 0.9324 - val_loss: 0.2435 - learning_rate: 1.4739e-07\n",
            "Epoch 174/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9799 - loss: 0.0967 - val_accuracy: 0.9319 - val_loss: 0.2564 - learning_rate: 1.4002e-07\n",
            "Epoch 175/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9787 - loss: 0.0947 - val_accuracy: 0.9296 - val_loss: 0.2981 - learning_rate: 1.3302e-07\n",
            "Epoch 176/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9815 - loss: 0.0877 - val_accuracy: 0.9384 - val_loss: 0.2068 - learning_rate: 1.2637e-07\n",
            "Epoch 177/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9793 - loss: 0.0927 - val_accuracy: 0.9278 - val_loss: 0.3450 - learning_rate: 1.2005e-07\n",
            "Epoch 178/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9816 - loss: 0.0861 - val_accuracy: 0.9324 - val_loss: 0.2462 - learning_rate: 1.1405e-07\n",
            "Epoch 179/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9822 - loss: 0.0862 - val_accuracy: 0.9305 - val_loss: 0.2918 - learning_rate: 1.0834e-07\n",
            "Epoch 180/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9814 - loss: 0.0903 - val_accuracy: 0.9292 - val_loss: 0.3232 - learning_rate: 1.0293e-07\n",
            "Epoch 181/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9819 - loss: 0.0902 - val_accuracy: 0.9259 - val_loss: 0.3660 - learning_rate: 9.7780e-08\n",
            "Epoch 182/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9814 - loss: 0.0891 - val_accuracy: 0.9246 - val_loss: 0.3776 - learning_rate: 9.2891e-08\n",
            "Epoch 183/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9810 - loss: 0.0908 - val_accuracy: 0.9328 - val_loss: 0.2354 - learning_rate: 8.8246e-08\n",
            "Epoch 184/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9816 - loss: 0.0898 - val_accuracy: 0.9402 - val_loss: 0.2052 - learning_rate: 8.3834e-08\n",
            "Epoch 185/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9824 - loss: 0.0871 - val_accuracy: 0.9301 - val_loss: 0.2645 - learning_rate: 7.9642e-08\n",
            "Epoch 186/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9830 - loss: 0.0885 - val_accuracy: 0.9305 - val_loss: 0.2916 - learning_rate: 7.5660e-08\n",
            "Epoch 187/200\n",
            "272/272 - 21s - 76ms/step - accuracy: 0.9826 - loss: 0.0868 - val_accuracy: 0.9333 - val_loss: 0.2302 - learning_rate: 7.1877e-08\n",
            "Epoch 188/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9815 - loss: 0.0903 - val_accuracy: 0.9351 - val_loss: 0.2166 - learning_rate: 6.8283e-08\n",
            "Epoch 189/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9838 - loss: 0.0852 - val_accuracy: 0.9430 - val_loss: 0.2039 - learning_rate: 6.4869e-08\n",
            "Epoch 190/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9811 - loss: 0.0897 - val_accuracy: 0.9328 - val_loss: 0.2379 - learning_rate: 6.1626e-08\n",
            "Epoch 191/200\n",
            "272/272 - 21s - 77ms/step - accuracy: 0.9819 - loss: 0.0883 - val_accuracy: 0.9310 - val_loss: 0.2693 - learning_rate: 5.8544e-08\n",
            "Epoch 192/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9822 - loss: 0.0888 - val_accuracy: 0.9328 - val_loss: 0.2352 - learning_rate: 5.5617e-08\n",
            "Epoch 193/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9814 - loss: 0.0847 - val_accuracy: 0.9411 - val_loss: 0.2049 - learning_rate: 5.2836e-08\n",
            "Epoch 194/200\n",
            "272/272 - 20s - 74ms/step - accuracy: 0.9825 - loss: 0.0880 - val_accuracy: 0.9333 - val_loss: 0.2222 - learning_rate: 5.0194e-08\n",
            "Epoch 195/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9841 - loss: 0.0876 - val_accuracy: 0.9305 - val_loss: 0.2775 - learning_rate: 4.7685e-08\n",
            "Epoch 196/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9806 - loss: 0.0906 - val_accuracy: 0.9535 - val_loss: 0.2004 - learning_rate: 4.5301e-08\n",
            "Epoch 197/200\n",
            "272/272 - 20s - 75ms/step - accuracy: 0.9817 - loss: 0.0869 - val_accuracy: 0.9328 - val_loss: 0.2496 - learning_rate: 4.3035e-08\n",
            "Epoch 198/200\n",
            "272/272 - 21s - 75ms/step - accuracy: 0.9811 - loss: 0.0862 - val_accuracy: 0.9338 - val_loss: 0.2175 - learning_rate: 4.0884e-08\n",
            "Epoch 199/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9810 - loss: 0.0909 - val_accuracy: 0.9333 - val_loss: 0.2164 - learning_rate: 3.8840e-08\n",
            "Epoch 200/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9810 - loss: 0.0917 - val_accuracy: 0.9305 - val_loss: 0.2881 - learning_rate: 3.6898e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_GRU_Dense_InceptionTime(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # CNN layers\n",
        "    x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001),  padding='same')(inputs)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001),  padding='same')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # InceptionTime modules\n",
        "    for _ in range(n_modules):\n",
        "        x = inception_module(x, n_filters=n_filters)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # GRU layer\n",
        "    x = layers.GRU(128, return_sequences=True)(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "CNN_GRU_Dense_model = CNN_GRU_Dense_InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "CNN_GRU_Dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "CNN_GRU_Dense_model.summary()\n",
        "teacher_hybrid_model = tf.keras.models.clone_model(CNN_GRU_Dense_model)\n",
        "teacher_hybrid_model_history = CNN_GRU_Dense_model.fit(X_train, y_train_, epochs=200, batch_size=32, verbose=2, validation_data=(X_val, y_val), callbacks=[annealer])\n",
        "\n"
      ],
      "metadata": {
        "id": "1Btpx0NTd3C8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11e3b14d-8fc0-4804-c97a-f6a2c6fe1e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_62 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m152\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_63 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m152\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m24,704\u001b[0m │ max_pooling1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_64 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_12             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_65 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_66 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_67 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_68 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_69 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_13             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_70 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_71 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_72 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_73 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_74 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_14             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_75 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_76 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_77 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_78 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_14            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_79 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_15             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_80 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_81 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_82 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_83 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_15            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_84 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_16             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_85 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_86 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_87 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_88 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_16            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_89 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m512\u001b[0m │ conv1d_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_17             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_90 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_91 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m49,280\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_92 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m82,048\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_93 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m16,512\u001b[0m │ max_pooling1d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_17            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ conv1d_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv1d_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv1d_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ concatenate_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m246,528\u001b[0m │ dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m16,512\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │          \u001b[38;5;34m1,161\u001b[0m │ dropout_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ max_pooling1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_12             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_13             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_14             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_14            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_15             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_15            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_16             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_16            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_17             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ max_pooling1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_17            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv1d_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv1d_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">246,528</span> │ dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │ dropout_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,623,177\u001b[0m (6.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,623,177</span> (6.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,621,641\u001b[0m (6.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,621,641</span> (6.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "272/272 - 35s - 129ms/step - accuracy: 0.6092 - loss: 2.0010 - val_accuracy: 0.6389 - val_loss: 1.6537 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "272/272 - 14s - 50ms/step - accuracy: 0.7792 - loss: 1.1320 - val_accuracy: 0.7507 - val_loss: 1.1054 - learning_rate: 9.5000e-04\n",
            "Epoch 3/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.8496 - loss: 0.7744 - val_accuracy: 0.8165 - val_loss: 0.7766 - learning_rate: 9.0250e-04\n",
            "Epoch 4/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.8751 - loss: 0.6133 - val_accuracy: 0.7098 - val_loss: 1.1219 - learning_rate: 8.5737e-04\n",
            "Epoch 5/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.8930 - loss: 0.5398 - val_accuracy: 0.8855 - val_loss: 0.5334 - learning_rate: 8.1451e-04\n",
            "Epoch 6/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9017 - loss: 0.4735 - val_accuracy: 0.8109 - val_loss: 0.6934 - learning_rate: 7.7378e-04\n",
            "Epoch 7/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9068 - loss: 0.4474 - val_accuracy: 0.9016 - val_loss: 0.3888 - learning_rate: 7.3509e-04\n",
            "Epoch 8/200\n",
            "272/272 - 9s - 35ms/step - accuracy: 0.9151 - loss: 0.4197 - val_accuracy: 0.7111 - val_loss: 0.8230 - learning_rate: 6.9834e-04\n",
            "Epoch 9/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9184 - loss: 0.3768 - val_accuracy: 0.9200 - val_loss: 0.3855 - learning_rate: 6.6342e-04\n",
            "Epoch 10/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9339 - loss: 0.3385 - val_accuracy: 0.9305 - val_loss: 0.3477 - learning_rate: 6.3025e-04\n",
            "Epoch 11/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9346 - loss: 0.3323 - val_accuracy: 0.9457 - val_loss: 0.2908 - learning_rate: 5.9874e-04\n",
            "Epoch 12/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9325 - loss: 0.3335 - val_accuracy: 0.8629 - val_loss: 0.4604 - learning_rate: 5.6880e-04\n",
            "Epoch 13/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9433 - loss: 0.2955 - val_accuracy: 0.9181 - val_loss: 0.3531 - learning_rate: 5.4036e-04\n",
            "Epoch 14/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9388 - loss: 0.2963 - val_accuracy: 0.9397 - val_loss: 0.2893 - learning_rate: 5.1334e-04\n",
            "Epoch 15/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9436 - loss: 0.2752 - val_accuracy: 0.8993 - val_loss: 0.4046 - learning_rate: 4.8767e-04\n",
            "Epoch 16/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9465 - loss: 0.2705 - val_accuracy: 0.9494 - val_loss: 0.2463 - learning_rate: 4.6329e-04\n",
            "Epoch 17/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9485 - loss: 0.2511 - val_accuracy: 0.9425 - val_loss: 0.3003 - learning_rate: 4.4013e-04\n",
            "Epoch 18/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9472 - loss: 0.2641 - val_accuracy: 0.9558 - val_loss: 0.2358 - learning_rate: 4.1812e-04\n",
            "Epoch 19/200\n",
            "272/272 - 19s - 71ms/step - accuracy: 0.9541 - loss: 0.2393 - val_accuracy: 0.9062 - val_loss: 0.3570 - learning_rate: 3.9721e-04\n",
            "Epoch 20/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9520 - loss: 0.2402 - val_accuracy: 0.9572 - val_loss: 0.2336 - learning_rate: 3.7735e-04\n",
            "Epoch 21/200\n",
            "272/272 - 12s - 43ms/step - accuracy: 0.9542 - loss: 0.2288 - val_accuracy: 0.9618 - val_loss: 0.2040 - learning_rate: 3.5849e-04\n",
            "Epoch 22/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9588 - loss: 0.2095 - val_accuracy: 0.9282 - val_loss: 0.3069 - learning_rate: 3.4056e-04\n",
            "Epoch 23/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9611 - loss: 0.2063 - val_accuracy: 0.9485 - val_loss: 0.2614 - learning_rate: 3.2353e-04\n",
            "Epoch 24/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9614 - loss: 0.2034 - val_accuracy: 0.9517 - val_loss: 0.2389 - learning_rate: 3.0736e-04\n",
            "Epoch 25/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9603 - loss: 0.2093 - val_accuracy: 0.9531 - val_loss: 0.2348 - learning_rate: 2.9199e-04\n",
            "Epoch 26/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9612 - loss: 0.1989 - val_accuracy: 0.9462 - val_loss: 0.2335 - learning_rate: 2.7739e-04\n",
            "Epoch 27/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9632 - loss: 0.1872 - val_accuracy: 0.9581 - val_loss: 0.2004 - learning_rate: 2.6352e-04\n",
            "Epoch 28/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9650 - loss: 0.1845 - val_accuracy: 0.9646 - val_loss: 0.1881 - learning_rate: 2.5034e-04\n",
            "Epoch 29/200\n",
            "272/272 - 20s - 73ms/step - accuracy: 0.9646 - loss: 0.1812 - val_accuracy: 0.9641 - val_loss: 0.1818 - learning_rate: 2.3783e-04\n",
            "Epoch 30/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9684 - loss: 0.1693 - val_accuracy: 0.9572 - val_loss: 0.2140 - learning_rate: 2.2594e-04\n",
            "Epoch 31/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9688 - loss: 0.1619 - val_accuracy: 0.9660 - val_loss: 0.1802 - learning_rate: 2.1464e-04\n",
            "Epoch 32/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9638 - loss: 0.1727 - val_accuracy: 0.9660 - val_loss: 0.1916 - learning_rate: 2.0391e-04\n",
            "Epoch 33/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9663 - loss: 0.1626 - val_accuracy: 0.9623 - val_loss: 0.1772 - learning_rate: 1.9371e-04\n",
            "Epoch 34/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9688 - loss: 0.1569 - val_accuracy: 0.9623 - val_loss: 0.1944 - learning_rate: 1.8403e-04\n",
            "Epoch 35/200\n",
            "272/272 - 12s - 43ms/step - accuracy: 0.9704 - loss: 0.1490 - val_accuracy: 0.9692 - val_loss: 0.1577 - learning_rate: 1.7482e-04\n",
            "Epoch 36/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9697 - loss: 0.1423 - val_accuracy: 0.9715 - val_loss: 0.1575 - learning_rate: 1.6608e-04\n",
            "Epoch 37/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9719 - loss: 0.1434 - val_accuracy: 0.9710 - val_loss: 0.1487 - learning_rate: 1.5778e-04\n",
            "Epoch 38/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9716 - loss: 0.1428 - val_accuracy: 0.9692 - val_loss: 0.1583 - learning_rate: 1.4989e-04\n",
            "Epoch 39/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9733 - loss: 0.1338 - val_accuracy: 0.9687 - val_loss: 0.1607 - learning_rate: 1.4240e-04\n",
            "Epoch 40/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9722 - loss: 0.1366 - val_accuracy: 0.9738 - val_loss: 0.1457 - learning_rate: 1.3528e-04\n",
            "Epoch 41/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9719 - loss: 0.1343 - val_accuracy: 0.9706 - val_loss: 0.1544 - learning_rate: 1.2851e-04\n",
            "Epoch 42/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9730 - loss: 0.1295 - val_accuracy: 0.9683 - val_loss: 0.1566 - learning_rate: 1.2209e-04\n",
            "Epoch 43/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9740 - loss: 0.1245 - val_accuracy: 0.9692 - val_loss: 0.1571 - learning_rate: 1.1598e-04\n",
            "Epoch 44/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9763 - loss: 0.1185 - val_accuracy: 0.9719 - val_loss: 0.1451 - learning_rate: 1.1018e-04\n",
            "Epoch 45/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9731 - loss: 0.1238 - val_accuracy: 0.9770 - val_loss: 0.1335 - learning_rate: 1.0467e-04\n",
            "Epoch 46/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9735 - loss: 0.1210 - val_accuracy: 0.9724 - val_loss: 0.1523 - learning_rate: 9.9440e-05\n",
            "Epoch 47/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9763 - loss: 0.1171 - val_accuracy: 0.9784 - val_loss: 0.1245 - learning_rate: 9.4468e-05\n",
            "Epoch 48/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9769 - loss: 0.1180 - val_accuracy: 0.9765 - val_loss: 0.1268 - learning_rate: 8.9745e-05\n",
            "Epoch 49/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9763 - loss: 0.1156 - val_accuracy: 0.9765 - val_loss: 0.1267 - learning_rate: 8.5258e-05\n",
            "Epoch 50/200\n",
            "272/272 - 11s - 42ms/step - accuracy: 0.9775 - loss: 0.1118 - val_accuracy: 0.9770 - val_loss: 0.1231 - learning_rate: 8.0995e-05\n",
            "Epoch 51/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9767 - loss: 0.1102 - val_accuracy: 0.9756 - val_loss: 0.1269 - learning_rate: 7.6945e-05\n",
            "Epoch 52/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9785 - loss: 0.1031 - val_accuracy: 0.9756 - val_loss: 0.1284 - learning_rate: 7.3098e-05\n",
            "Epoch 53/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9777 - loss: 0.1060 - val_accuracy: 0.9742 - val_loss: 0.1270 - learning_rate: 6.9443e-05\n",
            "Epoch 54/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9796 - loss: 0.1017 - val_accuracy: 0.9761 - val_loss: 0.1280 - learning_rate: 6.5971e-05\n",
            "Epoch 55/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9785 - loss: 0.1023 - val_accuracy: 0.9784 - val_loss: 0.1209 - learning_rate: 6.2672e-05\n",
            "Epoch 56/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9796 - loss: 0.0972 - val_accuracy: 0.9779 - val_loss: 0.1163 - learning_rate: 5.9539e-05\n",
            "Epoch 57/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9787 - loss: 0.1042 - val_accuracy: 0.9761 - val_loss: 0.1157 - learning_rate: 5.6562e-05\n",
            "Epoch 58/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9819 - loss: 0.0975 - val_accuracy: 0.9742 - val_loss: 0.1231 - learning_rate: 5.3734e-05\n",
            "Epoch 59/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9804 - loss: 0.0975 - val_accuracy: 0.9775 - val_loss: 0.1191 - learning_rate: 5.1047e-05\n",
            "Epoch 60/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9808 - loss: 0.0995 - val_accuracy: 0.9798 - val_loss: 0.1101 - learning_rate: 4.8495e-05\n",
            "Epoch 61/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9786 - loss: 0.0999 - val_accuracy: 0.9770 - val_loss: 0.1123 - learning_rate: 4.6070e-05\n",
            "Epoch 62/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9819 - loss: 0.0911 - val_accuracy: 0.9793 - val_loss: 0.1110 - learning_rate: 4.3766e-05\n",
            "Epoch 63/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9803 - loss: 0.0922 - val_accuracy: 0.9779 - val_loss: 0.1131 - learning_rate: 4.1578e-05\n",
            "Epoch 64/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9800 - loss: 0.0921 - val_accuracy: 0.9788 - val_loss: 0.1104 - learning_rate: 3.9499e-05\n",
            "Epoch 65/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9814 - loss: 0.0899 - val_accuracy: 0.9788 - val_loss: 0.1099 - learning_rate: 3.7524e-05\n",
            "Epoch 66/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9803 - loss: 0.0896 - val_accuracy: 0.9793 - val_loss: 0.1098 - learning_rate: 3.5648e-05\n",
            "Epoch 67/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9810 - loss: 0.0913 - val_accuracy: 0.9793 - val_loss: 0.1113 - learning_rate: 3.3866e-05\n",
            "Epoch 68/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9816 - loss: 0.0908 - val_accuracy: 0.9793 - val_loss: 0.1114 - learning_rate: 3.2172e-05\n",
            "Epoch 69/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9829 - loss: 0.0877 - val_accuracy: 0.9802 - val_loss: 0.1101 - learning_rate: 3.0564e-05\n",
            "Epoch 70/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9816 - loss: 0.0922 - val_accuracy: 0.9793 - val_loss: 0.1087 - learning_rate: 2.9035e-05\n",
            "Epoch 71/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9829 - loss: 0.0839 - val_accuracy: 0.9793 - val_loss: 0.1089 - learning_rate: 2.7584e-05\n",
            "Epoch 72/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9819 - loss: 0.0884 - val_accuracy: 0.9807 - val_loss: 0.1101 - learning_rate: 2.6205e-05\n",
            "Epoch 73/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9836 - loss: 0.0855 - val_accuracy: 0.9798 - val_loss: 0.1052 - learning_rate: 2.4894e-05\n",
            "Epoch 74/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9824 - loss: 0.0845 - val_accuracy: 0.9802 - val_loss: 0.1046 - learning_rate: 2.3650e-05\n",
            "Epoch 75/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9838 - loss: 0.0809 - val_accuracy: 0.9802 - val_loss: 0.1050 - learning_rate: 2.2467e-05\n",
            "Epoch 76/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9833 - loss: 0.0849 - val_accuracy: 0.9821 - val_loss: 0.1023 - learning_rate: 2.1344e-05\n",
            "Epoch 77/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9831 - loss: 0.0869 - val_accuracy: 0.9811 - val_loss: 0.1032 - learning_rate: 2.0277e-05\n",
            "Epoch 78/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9832 - loss: 0.0859 - val_accuracy: 0.9811 - val_loss: 0.1064 - learning_rate: 1.9263e-05\n",
            "Epoch 79/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9848 - loss: 0.0816 - val_accuracy: 0.9816 - val_loss: 0.1018 - learning_rate: 1.8300e-05\n",
            "Epoch 80/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9841 - loss: 0.0786 - val_accuracy: 0.9816 - val_loss: 0.1014 - learning_rate: 1.7385e-05\n",
            "Epoch 81/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9838 - loss: 0.0813 - val_accuracy: 0.9811 - val_loss: 0.1022 - learning_rate: 1.6515e-05\n",
            "Epoch 82/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9855 - loss: 0.0800 - val_accuracy: 0.9816 - val_loss: 0.1004 - learning_rate: 1.5690e-05\n",
            "Epoch 83/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9839 - loss: 0.0793 - val_accuracy: 0.9807 - val_loss: 0.1011 - learning_rate: 1.4905e-05\n",
            "Epoch 84/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9839 - loss: 0.0791 - val_accuracy: 0.9811 - val_loss: 0.1033 - learning_rate: 1.4160e-05\n",
            "Epoch 85/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9855 - loss: 0.0808 - val_accuracy: 0.9821 - val_loss: 0.1004 - learning_rate: 1.3452e-05\n",
            "Epoch 86/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9844 - loss: 0.0793 - val_accuracy: 0.9811 - val_loss: 0.0995 - learning_rate: 1.2779e-05\n",
            "Epoch 87/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9855 - loss: 0.0775 - val_accuracy: 0.9811 - val_loss: 0.1004 - learning_rate: 1.2140e-05\n",
            "Epoch 88/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9854 - loss: 0.0753 - val_accuracy: 0.9821 - val_loss: 0.1018 - learning_rate: 1.1533e-05\n",
            "Epoch 89/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9861 - loss: 0.0760 - val_accuracy: 0.9807 - val_loss: 0.1021 - learning_rate: 1.0957e-05\n",
            "Epoch 90/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9875 - loss: 0.0739 - val_accuracy: 0.9816 - val_loss: 0.1023 - learning_rate: 1.0409e-05\n",
            "Epoch 91/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9847 - loss: 0.0763 - val_accuracy: 0.9821 - val_loss: 0.1005 - learning_rate: 9.8884e-06\n",
            "Epoch 92/200\n",
            "272/272 - 9s - 35ms/step - accuracy: 0.9844 - loss: 0.0796 - val_accuracy: 0.9811 - val_loss: 0.1000 - learning_rate: 9.3939e-06\n",
            "Epoch 93/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9854 - loss: 0.0768 - val_accuracy: 0.9816 - val_loss: 0.1005 - learning_rate: 8.9242e-06\n",
            "Epoch 94/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9850 - loss: 0.0771 - val_accuracy: 0.9816 - val_loss: 0.1004 - learning_rate: 8.4780e-06\n",
            "Epoch 95/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9861 - loss: 0.0763 - val_accuracy: 0.9816 - val_loss: 0.0996 - learning_rate: 8.0541e-06\n",
            "Epoch 96/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9841 - loss: 0.0778 - val_accuracy: 0.9816 - val_loss: 0.0998 - learning_rate: 7.6514e-06\n",
            "Epoch 97/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9855 - loss: 0.0753 - val_accuracy: 0.9811 - val_loss: 0.0987 - learning_rate: 7.2689e-06\n",
            "Epoch 98/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9844 - loss: 0.0761 - val_accuracy: 0.9811 - val_loss: 0.1004 - learning_rate: 6.9054e-06\n",
            "Epoch 99/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9841 - loss: 0.0751 - val_accuracy: 0.9811 - val_loss: 0.0999 - learning_rate: 6.5601e-06\n",
            "Epoch 100/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9862 - loss: 0.0741 - val_accuracy: 0.9816 - val_loss: 0.1001 - learning_rate: 6.2321e-06\n",
            "Epoch 101/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9845 - loss: 0.0755 - val_accuracy: 0.9811 - val_loss: 0.1006 - learning_rate: 5.9205e-06\n",
            "Epoch 102/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9862 - loss: 0.0731 - val_accuracy: 0.9811 - val_loss: 0.1016 - learning_rate: 5.6245e-06\n",
            "Epoch 103/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9865 - loss: 0.0748 - val_accuracy: 0.9816 - val_loss: 0.1007 - learning_rate: 5.3433e-06\n",
            "Epoch 104/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9855 - loss: 0.0744 - val_accuracy: 0.9821 - val_loss: 0.0995 - learning_rate: 5.0761e-06\n",
            "Epoch 105/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9850 - loss: 0.0746 - val_accuracy: 0.9811 - val_loss: 0.0996 - learning_rate: 4.8223e-06\n",
            "Epoch 106/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9861 - loss: 0.0768 - val_accuracy: 0.9816 - val_loss: 0.1000 - learning_rate: 4.5812e-06\n",
            "Epoch 107/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9859 - loss: 0.0756 - val_accuracy: 0.9816 - val_loss: 0.0985 - learning_rate: 4.3521e-06\n",
            "Epoch 108/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9860 - loss: 0.0720 - val_accuracy: 0.9807 - val_loss: 0.0987 - learning_rate: 4.1345e-06\n",
            "Epoch 109/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9839 - loss: 0.0736 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 3.9278e-06\n",
            "Epoch 110/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9849 - loss: 0.0747 - val_accuracy: 0.9811 - val_loss: 0.0987 - learning_rate: 3.7314e-06\n",
            "Epoch 111/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9860 - loss: 0.0731 - val_accuracy: 0.9816 - val_loss: 0.0998 - learning_rate: 3.5448e-06\n",
            "Epoch 112/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9868 - loss: 0.0702 - val_accuracy: 0.9816 - val_loss: 0.0989 - learning_rate: 3.3676e-06\n",
            "Epoch 113/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9861 - loss: 0.0746 - val_accuracy: 0.9821 - val_loss: 0.0982 - learning_rate: 3.1992e-06\n",
            "Epoch 114/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9845 - loss: 0.0759 - val_accuracy: 0.9811 - val_loss: 0.0981 - learning_rate: 3.0393e-06\n",
            "Epoch 115/200\n",
            "272/272 - 11s - 42ms/step - accuracy: 0.9855 - loss: 0.0762 - val_accuracy: 0.9811 - val_loss: 0.0995 - learning_rate: 2.8873e-06\n",
            "Epoch 116/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9870 - loss: 0.0734 - val_accuracy: 0.9811 - val_loss: 0.0995 - learning_rate: 2.7429e-06\n",
            "Epoch 117/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9861 - loss: 0.0736 - val_accuracy: 0.9811 - val_loss: 0.0999 - learning_rate: 2.6058e-06\n",
            "Epoch 118/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9859 - loss: 0.0741 - val_accuracy: 0.9821 - val_loss: 0.0979 - learning_rate: 2.4755e-06\n",
            "Epoch 119/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9872 - loss: 0.0708 - val_accuracy: 0.9821 - val_loss: 0.0976 - learning_rate: 2.3517e-06\n",
            "Epoch 120/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9870 - loss: 0.0721 - val_accuracy: 0.9821 - val_loss: 0.0975 - learning_rate: 2.2341e-06\n",
            "Epoch 121/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9859 - loss: 0.0733 - val_accuracy: 0.9811 - val_loss: 0.0980 - learning_rate: 2.1224e-06\n",
            "Epoch 122/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9859 - loss: 0.0712 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 2.0163e-06\n",
            "Epoch 123/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9861 - loss: 0.0743 - val_accuracy: 0.9816 - val_loss: 0.0979 - learning_rate: 1.9155e-06\n",
            "Epoch 124/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9857 - loss: 0.0732 - val_accuracy: 0.9821 - val_loss: 0.0982 - learning_rate: 1.8197e-06\n",
            "Epoch 125/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9873 - loss: 0.0716 - val_accuracy: 0.9811 - val_loss: 0.0988 - learning_rate: 1.7287e-06\n",
            "Epoch 126/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9862 - loss: 0.0745 - val_accuracy: 0.9816 - val_loss: 0.0989 - learning_rate: 1.6423e-06\n",
            "Epoch 127/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9847 - loss: 0.0755 - val_accuracy: 0.9816 - val_loss: 0.0974 - learning_rate: 1.5602e-06\n",
            "Epoch 128/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9876 - loss: 0.0705 - val_accuracy: 0.9811 - val_loss: 0.0978 - learning_rate: 1.4822e-06\n",
            "Epoch 129/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9857 - loss: 0.0719 - val_accuracy: 0.9816 - val_loss: 0.0987 - learning_rate: 1.4081e-06\n",
            "Epoch 130/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9863 - loss: 0.0723 - val_accuracy: 0.9811 - val_loss: 0.0992 - learning_rate: 1.3377e-06\n",
            "Epoch 131/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9860 - loss: 0.0724 - val_accuracy: 0.9816 - val_loss: 0.0987 - learning_rate: 1.2708e-06\n",
            "Epoch 132/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9855 - loss: 0.0736 - val_accuracy: 0.9816 - val_loss: 0.0978 - learning_rate: 1.2072e-06\n",
            "Epoch 133/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9862 - loss: 0.0723 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 1.1469e-06\n",
            "Epoch 134/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9860 - loss: 0.0719 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 1.0895e-06\n",
            "Epoch 135/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9842 - loss: 0.0723 - val_accuracy: 0.9811 - val_loss: 0.0986 - learning_rate: 1.0351e-06\n",
            "Epoch 136/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9854 - loss: 0.0741 - val_accuracy: 0.9807 - val_loss: 0.0987 - learning_rate: 9.8330e-07\n",
            "Epoch 137/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9859 - loss: 0.0713 - val_accuracy: 0.9811 - val_loss: 0.0989 - learning_rate: 9.3414e-07\n",
            "Epoch 138/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9860 - loss: 0.0720 - val_accuracy: 0.9811 - val_loss: 0.0985 - learning_rate: 8.8743e-07\n",
            "Epoch 139/200\n",
            "272/272 - 18s - 67ms/step - accuracy: 0.9863 - loss: 0.0718 - val_accuracy: 0.9811 - val_loss: 0.0989 - learning_rate: 8.4306e-07\n",
            "Epoch 140/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9860 - loss: 0.0755 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 8.0091e-07\n",
            "Epoch 141/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9879 - loss: 0.0681 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 7.6086e-07\n",
            "Epoch 142/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9854 - loss: 0.0742 - val_accuracy: 0.9811 - val_loss: 0.0993 - learning_rate: 7.2282e-07\n",
            "Epoch 143/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9863 - loss: 0.0718 - val_accuracy: 0.9811 - val_loss: 0.0993 - learning_rate: 6.8668e-07\n",
            "Epoch 144/200\n",
            "272/272 - 19s - 71ms/step - accuracy: 0.9862 - loss: 0.0698 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 6.5234e-07\n",
            "Epoch 145/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9860 - loss: 0.0731 - val_accuracy: 0.9811 - val_loss: 0.0988 - learning_rate: 6.1973e-07\n",
            "Epoch 146/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9856 - loss: 0.0714 - val_accuracy: 0.9811 - val_loss: 0.0993 - learning_rate: 5.8874e-07\n",
            "Epoch 147/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9869 - loss: 0.0724 - val_accuracy: 0.9811 - val_loss: 0.0995 - learning_rate: 5.5930e-07\n",
            "Epoch 148/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9859 - loss: 0.0739 - val_accuracy: 0.9816 - val_loss: 0.0994 - learning_rate: 5.3134e-07\n",
            "Epoch 149/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9844 - loss: 0.0789 - val_accuracy: 0.9816 - val_loss: 0.0987 - learning_rate: 5.0477e-07\n",
            "Epoch 150/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9841 - loss: 0.0761 - val_accuracy: 0.9816 - val_loss: 0.0989 - learning_rate: 4.7953e-07\n",
            "Epoch 151/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9863 - loss: 0.0712 - val_accuracy: 0.9816 - val_loss: 0.0984 - learning_rate: 4.5555e-07\n",
            "Epoch 152/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9852 - loss: 0.0738 - val_accuracy: 0.9816 - val_loss: 0.0990 - learning_rate: 4.3278e-07\n",
            "Epoch 153/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9855 - loss: 0.0735 - val_accuracy: 0.9816 - val_loss: 0.0985 - learning_rate: 4.1114e-07\n",
            "Epoch 154/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9861 - loss: 0.0701 - val_accuracy: 0.9811 - val_loss: 0.0991 - learning_rate: 3.9058e-07\n",
            "Epoch 155/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9846 - loss: 0.0732 - val_accuracy: 0.9816 - val_loss: 0.0992 - learning_rate: 3.7105e-07\n",
            "Epoch 156/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9859 - loss: 0.0716 - val_accuracy: 0.9811 - val_loss: 0.0990 - learning_rate: 3.5250e-07\n",
            "Epoch 157/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9862 - loss: 0.0758 - val_accuracy: 0.9816 - val_loss: 0.0987 - learning_rate: 3.3487e-07\n",
            "Epoch 158/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9856 - loss: 0.0718 - val_accuracy: 0.9816 - val_loss: 0.0989 - learning_rate: 3.1813e-07\n",
            "Epoch 159/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9852 - loss: 0.0708 - val_accuracy: 0.9816 - val_loss: 0.0983 - learning_rate: 3.0222e-07\n",
            "Epoch 160/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9868 - loss: 0.0733 - val_accuracy: 0.9811 - val_loss: 0.0986 - learning_rate: 2.8711e-07\n",
            "Epoch 161/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9857 - loss: 0.0749 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 2.7276e-07\n",
            "Epoch 162/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9857 - loss: 0.0724 - val_accuracy: 0.9816 - val_loss: 0.0990 - learning_rate: 2.5912e-07\n",
            "Epoch 163/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9857 - loss: 0.0720 - val_accuracy: 0.9811 - val_loss: 0.0988 - learning_rate: 2.4616e-07\n",
            "Epoch 164/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9869 - loss: 0.0704 - val_accuracy: 0.9811 - val_loss: 0.0989 - learning_rate: 2.3386e-07\n",
            "Epoch 165/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9856 - loss: 0.0741 - val_accuracy: 0.9807 - val_loss: 0.0992 - learning_rate: 2.2216e-07\n",
            "Epoch 166/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9865 - loss: 0.0724 - val_accuracy: 0.9811 - val_loss: 0.0982 - learning_rate: 2.1105e-07\n",
            "Epoch 167/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9860 - loss: 0.0737 - val_accuracy: 0.9811 - val_loss: 0.0986 - learning_rate: 2.0050e-07\n",
            "Epoch 168/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9873 - loss: 0.0690 - val_accuracy: 0.9811 - val_loss: 0.0979 - learning_rate: 1.9048e-07\n",
            "Epoch 169/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9875 - loss: 0.0705 - val_accuracy: 0.9816 - val_loss: 0.0980 - learning_rate: 1.8095e-07\n",
            "Epoch 170/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9837 - loss: 0.0775 - val_accuracy: 0.9816 - val_loss: 0.0981 - learning_rate: 1.7191e-07\n",
            "Epoch 171/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9850 - loss: 0.0725 - val_accuracy: 0.9811 - val_loss: 0.0989 - learning_rate: 1.6331e-07\n",
            "Epoch 172/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9848 - loss: 0.0746 - val_accuracy: 0.9811 - val_loss: 0.0991 - learning_rate: 1.5514e-07\n",
            "Epoch 173/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9862 - loss: 0.0708 - val_accuracy: 0.9811 - val_loss: 0.1000 - learning_rate: 1.4739e-07\n",
            "Epoch 174/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9865 - loss: 0.0723 - val_accuracy: 0.9816 - val_loss: 0.0980 - learning_rate: 1.4002e-07\n",
            "Epoch 175/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9847 - loss: 0.0738 - val_accuracy: 0.9811 - val_loss: 0.0987 - learning_rate: 1.3302e-07\n",
            "Epoch 176/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9850 - loss: 0.0764 - val_accuracy: 0.9811 - val_loss: 0.0986 - learning_rate: 1.2637e-07\n",
            "Epoch 177/200\n",
            "272/272 - 10s - 35ms/step - accuracy: 0.9852 - loss: 0.0725 - val_accuracy: 0.9816 - val_loss: 0.0982 - learning_rate: 1.2005e-07\n",
            "Epoch 178/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9857 - loss: 0.0750 - val_accuracy: 0.9816 - val_loss: 0.0988 - learning_rate: 1.1405e-07\n",
            "Epoch 179/200\n",
            "272/272 - 9s - 31ms/step - accuracy: 0.9854 - loss: 0.0727 - val_accuracy: 0.9811 - val_loss: 0.0984 - learning_rate: 1.0834e-07\n",
            "Epoch 180/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9856 - loss: 0.0753 - val_accuracy: 0.9816 - val_loss: 0.0984 - learning_rate: 1.0293e-07\n",
            "Epoch 181/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9863 - loss: 0.0727 - val_accuracy: 0.9811 - val_loss: 0.0986 - learning_rate: 9.7780e-08\n",
            "Epoch 182/200\n",
            "272/272 - 9s - 35ms/step - accuracy: 0.9852 - loss: 0.0731 - val_accuracy: 0.9811 - val_loss: 0.0988 - learning_rate: 9.2891e-08\n",
            "Epoch 183/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9860 - loss: 0.0712 - val_accuracy: 0.9811 - val_loss: 0.0989 - learning_rate: 8.8246e-08\n",
            "Epoch 184/200\n",
            "272/272 - 11s - 42ms/step - accuracy: 0.9861 - loss: 0.0727 - val_accuracy: 0.9816 - val_loss: 0.0981 - learning_rate: 8.3834e-08\n",
            "Epoch 185/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9861 - loss: 0.0732 - val_accuracy: 0.9816 - val_loss: 0.0986 - learning_rate: 7.9642e-08\n",
            "Epoch 186/200\n",
            "272/272 - 9s - 35ms/step - accuracy: 0.9865 - loss: 0.0705 - val_accuracy: 0.9811 - val_loss: 0.0985 - learning_rate: 7.5660e-08\n",
            "Epoch 187/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9850 - loss: 0.0780 - val_accuracy: 0.9811 - val_loss: 0.0987 - learning_rate: 7.1877e-08\n",
            "Epoch 188/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9864 - loss: 0.0730 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 6.8283e-08\n",
            "Epoch 189/200\n",
            "272/272 - 11s - 39ms/step - accuracy: 0.9859 - loss: 0.0699 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 6.4869e-08\n",
            "Epoch 190/200\n",
            "272/272 - 9s - 34ms/step - accuracy: 0.9860 - loss: 0.0731 - val_accuracy: 0.9811 - val_loss: 0.0992 - learning_rate: 6.1626e-08\n",
            "Epoch 191/200\n",
            "272/272 - 9s - 33ms/step - accuracy: 0.9864 - loss: 0.0734 - val_accuracy: 0.9816 - val_loss: 0.0986 - learning_rate: 5.8544e-08\n",
            "Epoch 192/200\n",
            "272/272 - 11s - 40ms/step - accuracy: 0.9867 - loss: 0.0717 - val_accuracy: 0.9816 - val_loss: 0.0983 - learning_rate: 5.5617e-08\n",
            "Epoch 193/200\n",
            "272/272 - 11s - 41ms/step - accuracy: 0.9857 - loss: 0.0718 - val_accuracy: 0.9811 - val_loss: 0.0983 - learning_rate: 5.2836e-08\n",
            "Epoch 194/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9869 - loss: 0.0748 - val_accuracy: 0.9811 - val_loss: 0.0987 - learning_rate: 5.0194e-08\n",
            "Epoch 195/200\n",
            "272/272 - 10s - 37ms/step - accuracy: 0.9869 - loss: 0.0697 - val_accuracy: 0.9816 - val_loss: 0.0977 - learning_rate: 4.7685e-08\n",
            "Epoch 196/200\n",
            "272/272 - 11s - 42ms/step - accuracy: 0.9868 - loss: 0.0692 - val_accuracy: 0.9816 - val_loss: 0.0981 - learning_rate: 4.5301e-08\n",
            "Epoch 197/200\n",
            "272/272 - 10s - 36ms/step - accuracy: 0.9862 - loss: 0.0731 - val_accuracy: 0.9811 - val_loss: 0.0991 - learning_rate: 4.3035e-08\n",
            "Epoch 198/200\n",
            "272/272 - 9s - 32ms/step - accuracy: 0.9868 - loss: 0.0718 - val_accuracy: 0.9816 - val_loss: 0.0982 - learning_rate: 4.0884e-08\n",
            "Epoch 199/200\n",
            "272/272 - 10s - 38ms/step - accuracy: 0.9860 - loss: 0.0724 - val_accuracy: 0.9811 - val_loss: 0.0984 - learning_rate: 3.8840e-08\n",
            "Epoch 200/200\n",
            "272/272 - 11s - 42ms/step - accuracy: 0.9834 - loss: 0.0731 - val_accuracy: 0.9816 - val_loss: 0.0978 - learning_rate: 3.6898e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_Inception(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    for _ in range(n_modules):\n",
        "        # Inception module with GRU\n",
        "        bottleneck = layers.Conv1D(n_filters, kernel_size=1, padding='same')(x)\n",
        "        bottleneck = layers.BatchNormalization()(bottleneck)\n",
        "        bottleneck = layers.Activation('relu')(bottleneck)\n",
        "\n",
        "\n",
        "        # Apply GRU to each branch\n",
        "        conv1 = GRU(n_filters//6, return_sequences=True)(bottleneck)\n",
        "        conv3 = GRU(n_filters//4, return_sequences=True)(bottleneck)\n",
        "        conv5 = GRU(n_filters//2, return_sequences=True)(bottleneck)\n",
        "        maxpool = GRU(n_filters, return_sequences=True)(bottleneck)\n",
        "\n",
        "        # Concatenate\n",
        "        x = layers.concatenate([conv1, conv3, conv5, maxpool], axis=-1)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = Reshape((1, 245))(x)\n",
        "\n",
        "    x = layers.Bidirectional(GRU(n_filters, return_sequences=False))(x)\n",
        "\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "gru_model = GRU_Inception(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "gru_model.summary()\n",
        "gru_clone = tf.keras.models.clone_model(gru_model)\n",
        "gru_clone_history = gru_model.fit(X_train, y_train_, epochs=200, batch_size=32, verbose=2, validation_data=(X_val, y_val), callbacks=[annealer])\n"
      ],
      "metadata": {
        "id": "WWvv_MUldmXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c76eed63-ea48-4aa6-94c2-093cd15151cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_18             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_18            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ gru_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ gru_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">31,488</span> │ dropout_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_19    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_19             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_19            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ gru_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ gru_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">31,488</span> │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_20    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_20             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_20            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">31,488</span> │ dropout_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_21    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_21             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_21            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">31,488</span> │ dropout_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_22    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_22             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_22            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">31,488</span> │ dropout_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_23    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_23             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,513</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15,552</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37,248</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │ activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_23            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ gru_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ gru_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">288,000</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,313</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_94 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_18             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_18            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ gru_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ gru_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_38 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_95 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m31,488\u001b[0m │ dropout_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_19    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_19             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_6 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_7 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_8 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_9 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_19            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ gru_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ gru_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_96 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m31,488\u001b[0m │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_20    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_20             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_10 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_11 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_12 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_13 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_20            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_97 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m31,488\u001b[0m │ dropout_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_21    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_21             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_14 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_15 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_16 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_17 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_21            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_98 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m31,488\u001b[0m │ dropout_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_22    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_22             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_18 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_19 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_20 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_21 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_22            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_42 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_99 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m31,488\u001b[0m │ dropout_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ batch_normalization_23    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ conv1d_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_23             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_22 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │          \u001b[38;5;34m9,513\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_23 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m15,552\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_24 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m37,248\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gru_25 (\u001b[38;5;33mGRU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m99,072\u001b[0m │ activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_23            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ gru_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ gru_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ gru_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_43 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m, \u001b[38;5;34m245\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ concatenate_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m245\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dropout_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m245\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m288,000\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │          \u001b[38;5;34m2,313\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,391</span> (5.41 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,419,391\u001b[0m (5.41 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,417,855</span> (5.41 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,417,855\u001b[0m (5.41 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "272/272 - 90s - 329ms/step - accuracy: 0.7904 - loss: 0.6397 - val_accuracy: 0.7443 - val_loss: 0.8394 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "272/272 - 71s - 262ms/step - accuracy: 0.9004 - loss: 0.3202 - val_accuracy: 0.7682 - val_loss: 0.6433 - learning_rate: 9.5000e-04\n",
            "Epoch 3/200\n",
            "272/272 - 78s - 287ms/step - accuracy: 0.9388 - loss: 0.2069 - val_accuracy: 0.9545 - val_loss: 0.1734 - learning_rate: 9.0250e-04\n",
            "Epoch 4/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9509 - loss: 0.1744 - val_accuracy: 0.8602 - val_loss: 0.4649 - learning_rate: 8.5737e-04\n",
            "Epoch 5/200\n",
            "272/272 - 81s - 298ms/step - accuracy: 0.9605 - loss: 0.1353 - val_accuracy: 0.9434 - val_loss: 0.1998 - learning_rate: 8.1451e-04\n",
            "Epoch 6/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9670 - loss: 0.1137 - val_accuracy: 0.8316 - val_loss: 0.4123 - learning_rate: 7.7378e-04\n",
            "Epoch 7/200\n",
            "272/272 - 83s - 305ms/step - accuracy: 0.9683 - loss: 0.1097 - val_accuracy: 0.9508 - val_loss: 0.2031 - learning_rate: 7.3509e-04\n",
            "Epoch 8/200\n",
            "272/272 - 82s - 300ms/step - accuracy: 0.9708 - loss: 0.0997 - val_accuracy: 0.8169 - val_loss: 0.5069 - learning_rate: 6.9834e-04\n",
            "Epoch 9/200\n",
            "272/272 - 67s - 246ms/step - accuracy: 0.9765 - loss: 0.0804 - val_accuracy: 0.7764 - val_loss: 0.9099 - learning_rate: 6.6342e-04\n",
            "Epoch 10/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9780 - loss: 0.0696 - val_accuracy: 0.8528 - val_loss: 0.4860 - learning_rate: 6.3025e-04\n",
            "Epoch 11/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9786 - loss: 0.0672 - val_accuracy: 0.9687 - val_loss: 0.1084 - learning_rate: 5.9874e-04\n",
            "Epoch 12/200\n",
            "272/272 - 67s - 247ms/step - accuracy: 0.9799 - loss: 0.0626 - val_accuracy: 0.8284 - val_loss: 0.5005 - learning_rate: 5.6880e-04\n",
            "Epoch 13/200\n",
            "272/272 - 86s - 317ms/step - accuracy: 0.9821 - loss: 0.0583 - val_accuracy: 0.9664 - val_loss: 0.1200 - learning_rate: 5.4036e-04\n",
            "Epoch 14/200\n",
            "272/272 - 78s - 286ms/step - accuracy: 0.9801 - loss: 0.0645 - val_accuracy: 0.9724 - val_loss: 0.1006 - learning_rate: 5.1334e-04\n",
            "Epoch 15/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9879 - loss: 0.0387 - val_accuracy: 0.9830 - val_loss: 0.0847 - learning_rate: 4.8767e-04\n",
            "Epoch 16/200\n",
            "272/272 - 72s - 264ms/step - accuracy: 0.9865 - loss: 0.0385 - val_accuracy: 0.9798 - val_loss: 0.0826 - learning_rate: 4.6329e-04\n",
            "Epoch 17/200\n",
            "272/272 - 77s - 284ms/step - accuracy: 0.9894 - loss: 0.0357 - val_accuracy: 0.9816 - val_loss: 0.0729 - learning_rate: 4.4013e-04\n",
            "Epoch 18/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9891 - loss: 0.0310 - val_accuracy: 0.9784 - val_loss: 0.0850 - learning_rate: 4.1812e-04\n",
            "Epoch 19/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9907 - loss: 0.0305 - val_accuracy: 0.9246 - val_loss: 0.2250 - learning_rate: 3.9721e-04\n",
            "Epoch 20/200\n",
            "272/272 - 82s - 300ms/step - accuracy: 0.9875 - loss: 0.0407 - val_accuracy: 0.8560 - val_loss: 0.5850 - learning_rate: 3.7735e-04\n",
            "Epoch 21/200\n",
            "272/272 - 67s - 246ms/step - accuracy: 0.9915 - loss: 0.0256 - val_accuracy: 0.9802 - val_loss: 0.0860 - learning_rate: 3.5849e-04\n",
            "Epoch 22/200\n",
            "272/272 - 67s - 245ms/step - accuracy: 0.9909 - loss: 0.0252 - val_accuracy: 0.9701 - val_loss: 0.1141 - learning_rate: 3.4056e-04\n",
            "Epoch 23/200\n",
            "272/272 - 87s - 320ms/step - accuracy: 0.9918 - loss: 0.0233 - val_accuracy: 0.9733 - val_loss: 0.1113 - learning_rate: 3.2353e-04\n",
            "Epoch 24/200\n",
            "272/272 - 81s - 299ms/step - accuracy: 0.9915 - loss: 0.0258 - val_accuracy: 0.9862 - val_loss: 0.0709 - learning_rate: 3.0736e-04\n",
            "Epoch 25/200\n",
            "272/272 - 71s - 262ms/step - accuracy: 0.9933 - loss: 0.0197 - val_accuracy: 0.9834 - val_loss: 0.0703 - learning_rate: 2.9199e-04\n",
            "Epoch 26/200\n",
            "272/272 - 78s - 286ms/step - accuracy: 0.9939 - loss: 0.0192 - val_accuracy: 0.9811 - val_loss: 0.0761 - learning_rate: 2.7739e-04\n",
            "Epoch 27/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9928 - loss: 0.0219 - val_accuracy: 0.8970 - val_loss: 0.2906 - learning_rate: 2.6352e-04\n",
            "Epoch 28/200\n",
            "272/272 - 86s - 315ms/step - accuracy: 0.9954 - loss: 0.0135 - val_accuracy: 0.9775 - val_loss: 0.0969 - learning_rate: 2.5034e-04\n",
            "Epoch 29/200\n",
            "272/272 - 78s - 288ms/step - accuracy: 0.9960 - loss: 0.0126 - val_accuracy: 0.9742 - val_loss: 0.1072 - learning_rate: 2.3783e-04\n",
            "Epoch 30/200\n",
            "272/272 - 67s - 248ms/step - accuracy: 0.9961 - loss: 0.0116 - val_accuracy: 0.9844 - val_loss: 0.0758 - learning_rate: 2.2594e-04\n",
            "Epoch 31/200\n",
            "272/272 - 82s - 300ms/step - accuracy: 0.9960 - loss: 0.0126 - val_accuracy: 0.9807 - val_loss: 0.0869 - learning_rate: 2.1464e-04\n",
            "Epoch 32/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9972 - loss: 0.0091 - val_accuracy: 0.9545 - val_loss: 0.1730 - learning_rate: 2.0391e-04\n",
            "Epoch 33/200\n",
            "272/272 - 72s - 263ms/step - accuracy: 0.9970 - loss: 0.0081 - val_accuracy: 0.9701 - val_loss: 0.1464 - learning_rate: 1.9371e-04\n",
            "Epoch 34/200\n",
            "272/272 - 71s - 261ms/step - accuracy: 0.9967 - loss: 0.0096 - val_accuracy: 0.9816 - val_loss: 0.0793 - learning_rate: 1.8403e-04\n",
            "Epoch 35/200\n",
            "272/272 - 79s - 289ms/step - accuracy: 0.9957 - loss: 0.0120 - val_accuracy: 0.9816 - val_loss: 0.0909 - learning_rate: 1.7482e-04\n",
            "Epoch 36/200\n",
            "272/272 - 81s - 299ms/step - accuracy: 0.9965 - loss: 0.0114 - val_accuracy: 0.9788 - val_loss: 0.0948 - learning_rate: 1.6608e-04\n",
            "Epoch 37/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9963 - loss: 0.0095 - val_accuracy: 0.9761 - val_loss: 0.1079 - learning_rate: 1.5778e-04\n",
            "Epoch 38/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9963 - loss: 0.0094 - val_accuracy: 0.9784 - val_loss: 0.1038 - learning_rate: 1.4989e-04\n",
            "Epoch 39/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9980 - loss: 0.0061 - val_accuracy: 0.9738 - val_loss: 0.1081 - learning_rate: 1.4240e-04\n",
            "Epoch 40/200\n",
            "272/272 - 81s - 299ms/step - accuracy: 0.9976 - loss: 0.0063 - val_accuracy: 0.9825 - val_loss: 0.0866 - learning_rate: 1.3528e-04\n",
            "Epoch 41/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9980 - loss: 0.0066 - val_accuracy: 0.9618 - val_loss: 0.1733 - learning_rate: 1.2851e-04\n",
            "Epoch 42/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9978 - loss: 0.0069 - val_accuracy: 0.9830 - val_loss: 0.0916 - learning_rate: 1.2209e-04\n",
            "Epoch 43/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 0.9494 - val_loss: 0.1827 - learning_rate: 1.1598e-04\n",
            "Epoch 44/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9983 - loss: 0.0067 - val_accuracy: 0.8951 - val_loss: 0.4308 - learning_rate: 1.1018e-04\n",
            "Epoch 45/200\n",
            "272/272 - 67s - 246ms/step - accuracy: 0.9976 - loss: 0.0077 - val_accuracy: 0.9678 - val_loss: 0.1559 - learning_rate: 1.0467e-04\n",
            "Epoch 46/200\n",
            "272/272 - 71s - 261ms/step - accuracy: 0.9978 - loss: 0.0074 - val_accuracy: 0.9871 - val_loss: 0.0682 - learning_rate: 9.9440e-05\n",
            "Epoch 47/200\n",
            "272/272 - 83s - 304ms/step - accuracy: 0.9982 - loss: 0.0047 - val_accuracy: 0.9761 - val_loss: 0.1206 - learning_rate: 9.4468e-05\n",
            "Epoch 48/200\n",
            "272/272 - 67s - 248ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.9604 - val_loss: 0.1965 - learning_rate: 8.9745e-05\n",
            "Epoch 49/200\n",
            "272/272 - 85s - 313ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9811 - val_loss: 0.0923 - learning_rate: 8.5258e-05\n",
            "Epoch 50/200\n",
            "272/272 - 78s - 287ms/step - accuracy: 0.9985 - loss: 0.0055 - val_accuracy: 0.9830 - val_loss: 0.0834 - learning_rate: 8.0995e-05\n",
            "Epoch 51/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9985 - loss: 0.0037 - val_accuracy: 0.9788 - val_loss: 0.1035 - learning_rate: 7.6945e-05\n",
            "Epoch 52/200\n",
            "272/272 - 82s - 303ms/step - accuracy: 0.9985 - loss: 0.0038 - val_accuracy: 0.9839 - val_loss: 0.0822 - learning_rate: 7.3098e-05\n",
            "Epoch 53/200\n",
            "272/272 - 71s - 261ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 0.9609 - val_loss: 0.1745 - learning_rate: 6.9443e-05\n",
            "Epoch 54/200\n",
            "272/272 - 67s - 247ms/step - accuracy: 0.9985 - loss: 0.0045 - val_accuracy: 0.9816 - val_loss: 0.0893 - learning_rate: 6.5971e-05\n",
            "Epoch 55/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9985 - loss: 0.0034 - val_accuracy: 0.9825 - val_loss: 0.0895 - learning_rate: 6.2672e-05\n",
            "Epoch 56/200\n",
            "272/272 - 81s - 300ms/step - accuracy: 0.9990 - loss: 0.0025 - val_accuracy: 0.9696 - val_loss: 0.1491 - learning_rate: 5.9539e-05\n",
            "Epoch 57/200\n",
            "272/272 - 83s - 304ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 0.9775 - val_loss: 0.1154 - learning_rate: 5.6562e-05\n",
            "Epoch 58/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9988 - loss: 0.0035 - val_accuracy: 0.9807 - val_loss: 0.1006 - learning_rate: 5.3734e-05\n",
            "Epoch 59/200\n",
            "272/272 - 68s - 248ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9811 - val_loss: 0.0946 - learning_rate: 5.1047e-05\n",
            "Epoch 60/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9990 - loss: 0.0030 - val_accuracy: 0.9821 - val_loss: 0.0852 - learning_rate: 4.8495e-05\n",
            "Epoch 61/200\n",
            "272/272 - 81s - 299ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 0.9798 - val_loss: 0.1055 - learning_rate: 4.6070e-05\n",
            "Epoch 62/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9988 - loss: 0.0026 - val_accuracy: 0.9825 - val_loss: 0.0827 - learning_rate: 4.3766e-05\n",
            "Epoch 63/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.8951 - val_loss: 0.3411 - learning_rate: 4.1578e-05\n",
            "Epoch 64/200\n",
            "272/272 - 82s - 301ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 0.9816 - val_loss: 0.0932 - learning_rate: 3.9499e-05\n",
            "Epoch 65/200\n",
            "272/272 - 82s - 300ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.9802 - val_loss: 0.1082 - learning_rate: 3.7524e-05\n",
            "Epoch 66/200\n",
            "272/272 - 82s - 302ms/step - accuracy: 0.9991 - loss: 0.0024 - val_accuracy: 0.9830 - val_loss: 0.0851 - learning_rate: 3.5648e-05\n",
            "Epoch 67/200\n",
            "272/272 - 82s - 303ms/step - accuracy: 0.9988 - loss: 0.0033 - val_accuracy: 0.9784 - val_loss: 0.1155 - learning_rate: 3.3866e-05\n",
            "Epoch 68/200\n",
            "272/272 - 82s - 300ms/step - accuracy: 0.9988 - loss: 0.0027 - val_accuracy: 0.9508 - val_loss: 0.1936 - learning_rate: 3.2172e-05\n",
            "Epoch 69/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Average, Maximum\n",
        "\n",
        "def ensemble_model(input_shape, num_classes=9, n_filters=32, n_modules=2):\n",
        "\n",
        "    # Create a functional API model for the ensemble\n",
        "    inputs = Input(shape=input_shape)\n",
        "    outputs = Maximum()([gru_model(inputs), Inceptionmodel(inputs), CNN_GRU_Dense_model(inputs)])\n",
        "\n",
        "    ensemble_model = Model(inputs=inputs, outputs=outputs)\n",
        "    ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    ensemble_model.summary()\n",
        "\n",
        "    return ensemble_model\n",
        "\n",
        "mnet = ensemble_model(input_shape, num_classes=9, n_filters=32, n_modules=2)"
      ],
      "metadata": {
        "id": "Np94xeA_desP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "# Define callbacks list\n",
        "callbacks = [early_stopping, annealer]#\n",
        "X_train, X_val, y_train_, y_val = train_test_split(dataX.values, y_train, test_size = 0.2, stratify=y_train, random_state=42)\n"
      ],
      "metadata": {
        "id": "21zxDLYIh0YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "def train_with_cross_val(model, X_train, y_train, fold=5):\n",
        "    kf = KFold(n_splits=fold, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for train_index, val_index in kf.split(X_train):\n",
        "        x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        #model.fit(x_train_fold,y_train_fold)\n",
        "        #callbacks = [early_stopping, annealer]\n",
        "\n",
        "        model.fit(x_train_fold,y_train_fold, epochs=100, batch_size=128, verbose=2, validation_data=(x_val_fold, y_val_fold))\n",
        "        y_pre_ = model.predict(x_val_fold)\n",
        "        accuracy = np.mean(y_pre_ == y_val_fold)\n",
        "        metrics.append(accuracy)\n",
        "        print(f'Accuracy={accuracy}')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "metr = train_with_cross_val(mnet, X_train, y_train_, fold=5)"
      ],
      "metadata": {
        "id": "fYjieVr4hYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the checkpoint path\n",
        "checkpoint_path = \"model_checkpoints/mtenet_max.weights.h5\" # Changed the file extension to .weights.h5\n",
        "#checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/model_checkpoints/cp.ckpt\"\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "mtenet_history = mnet.fit(X_train, y_train_, epochs=200, batch_size=32, verbose=2, validation_data=(X_val, y_val), callbacks=[annealer, cp_callback])\n",
        "# Plot the training history\n",
        "plot_(mtenet_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(mnet, X_val, y_val)\n",
        "plot_roc(mnet,X_val,y_val)"
      ],
      "metadata": {
        "id": "0BMWL6ojiYVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4f2GBQ1Oc0c"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plot_(m_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(model, X_val, y_val)\n",
        "plot_roc(model,X_val,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG6Z2P9QO11k"
      },
      "outputs": [],
      "source": [
        "def inception_module(input_tensor, n_filters=128):\n",
        "    # Bottleneck layer\n",
        "    bottleneck = layers.Conv1D(n_filters, kernel_size=1, padding='same')(input_tensor)\n",
        "    bottleneck = layers.BatchNormalization()(bottleneck)\n",
        "    bottleneck = layers.Activation('relu')(bottleneck)\n",
        "\n",
        "    # Inception layers\n",
        "    conv1 = layers.Conv1D(n_filters, kernel_size=1, padding='same')(bottleneck)\n",
        "    conv3 = layers.Conv1D(n_filters, kernel_size=3, padding='same')(bottleneck)\n",
        "    conv5 = layers.Conv1D(n_filters, kernel_size=5, padding='same')(bottleneck)\n",
        "    maxpool = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(bottleneck)\n",
        "    maxpool = layers.Conv1D(n_filters, kernel_size=1, padding='same')(maxpool)\n",
        "\n",
        "    # Add residual connection\n",
        "    residual = layers.Conv1D(n_filters * 4, kernel_size=1, padding='same')(input_tensor)\n",
        "    residual = layers.BatchNormalization()(residual)\n",
        "\n",
        "    # Concatenate\n",
        "    output = layers.concatenate([conv1, conv3, conv5, maxpool], axis=-1)\n",
        "    output = layers.Add()([output, residual])\n",
        "    output = layers.Activation('relu')(output)\n",
        "    return output\n",
        "\n",
        "def InceptionTime(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    for _ in range(n_modules):\n",
        "        x = inception_module(x, n_filters=n_filters)\n",
        "        x = layers.Dropout(0.2)(x)  # Add dropout for regularization\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)  # Add a dense layer\n",
        "    x = layers.Dropout(0.5)(x)  # Add dropout for regularization\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "base_model = InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=8)\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yObcJf9GPSUV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint path\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/base_model.weights.h5\"\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "base_history = base_model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2,\n",
        "                      validation_data=(X_val, y_val), callbacks=[annealer, cp_callback])\n",
        "\n",
        "#base_history=base_model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2, validation_data=(X_val, y_val), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8LYKWfTPJRK"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plot_(base_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(base_model, X_val, y_val)\n",
        "plot_roc(base_model,X_val,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZRVtqCnTHeS"
      },
      "outputs": [],
      "source": [
        "# prompt: save train model with check points to be used later\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint path\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/model_cp.weights.h5\"\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "m_history = model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2,\n",
        "                      validation_data=(X_val, y_val), callbacks=[annealer, cp_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wycnWC9ejb8e"
      },
      "outputs": [],
      "source": [
        "def CNN_GRU_Dense_InceptionTime(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # CNN layers\n",
        "    x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # InceptionTime modules\n",
        "    for _ in range(n_modules):\n",
        "        x = inception_module(x, n_filters=n_filters)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # GRU layer\n",
        "    x = layers.GRU(128, return_sequences=True)(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "CNN_GRU_Dense_model = CNN_GRU_Dense_InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "CNN_GRU_Dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "CNN_GRU_Dense_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWUAuyBfkDDA"
      },
      "outputs": [],
      "source": [
        "# Define the checkpoint path\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/model_checkpoints/cp_incep_cgru.weights.h5\"\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "\n",
        "CNN_GRU_Dense_model_history=CNN_GRU_Dense_model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2, validation_data=(X_val, y_val), callbacks=[annealer, cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l16t2MUkNwc"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plot_(CNN_GRU_Dense_model_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(CNN_GRU_Dense_model, X_val, y_val)\n",
        "plot_roc(CNN_GRU_Dense_model,X_val,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHEKIL03psgi"
      },
      "outputs": [],
      "source": [
        "def GRU_InceptionTime(input_shape, n_classes=2, n_filters=128, n_modules=6):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    for _ in range(n_modules):\n",
        "        # Inception module with GRU\n",
        "        bottleneck = layers.Conv1D(n_filters, kernel_size=1, padding='same')(x)\n",
        "        bottleneck = layers.BatchNormalization()(bottleneck)\n",
        "        bottleneck = layers.Activation('relu')(bottleneck)\n",
        "\n",
        "        conv1 = layers.Conv1D(n_filters, kernel_size=1, padding='same')(bottleneck)\n",
        "        conv3 = layers.Conv1D(n_filters, kernel_size=3, padding='same')(bottleneck)\n",
        "        conv5 = layers.Conv1D(n_filters, kernel_size=5, padding='same')(bottleneck)\n",
        "        maxpool = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(bottleneck)\n",
        "        maxpool = layers.Conv1D(n_filters, kernel_size=1, padding='same')(maxpool)\n",
        "\n",
        "        # Apply GRU to each branch\n",
        "        conv1 = layers.Bidirectional(GRU(n_filters // 2, return_sequences=True))(conv1)\n",
        "        conv3 = layers.Bidirectional(GRU(n_filters // 2, return_sequences=True))(conv3)\n",
        "        conv5 = layers.Bidirectional(GRU(n_filters // 2, return_sequences=True))(conv5)\n",
        "        maxpool = layers.Bidirectional(GRU(n_filters // 2, return_sequences=True))(maxpool)\n",
        "\n",
        "        # Concatenate\n",
        "        x = layers.concatenate([conv1, conv3, conv5, maxpool], axis=-1)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (305, 1)  # Example input shape: 100 time steps, 1 feature\n",
        "n_classes = 9  # Number of classes for classification\n",
        "gru_model = GRU_InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "gru_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox7WMo-WpsRs"
      },
      "outputs": [],
      "source": [
        "# Define the checkpoint path\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/gru_cp.weight.h5\"\n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                 save_weights_only=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "gru_history = gru_model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2,\n",
        "                      validation_data=(X_val, y_val), callbacks=[annealer, cp_callback])\n",
        "#gru_history=gru_model.fit(X_train, y_train_, epochs=100, batch_size=128, verbose=2, validation_data=(X_val, y_val), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cviizHStrYuZ"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plot_(gru_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(gru_model, X_val, y_val)\n",
        "plot_roc(gru_model,X_val,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgjj86qdSpko"
      },
      "outputs": [],
      "source": [
        "def ensemble_model(input_shape, num_classes=9, n_filters=32, n_modules=6):\n",
        "    # Create individual models\n",
        "    inception_gru_model = build_inceptionGRU_model(input_shape, num_classes, n_filters, n_modules)\n",
        "    cnn_gru_dense_inception_time_model = CNN_GRU_Dense_InceptionTime(input_shape, num_classes, n_filters, n_modules)\n",
        "    inception_time_model = base_model(input_shape, num_classes, n_filters, n_modules)\n",
        "\n",
        "    # Compile each model with a common optimizer and loss function\n",
        "    optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed\n",
        "    loss_function = CategoricalCrossentropy()  # Replace with appropriate loss if necessary\n",
        "\n",
        "    inception_gru_model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "    cnn_gru_dense_inception_time_model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "    inception_time_model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "    # Create a functional API model for the ensemble\n",
        "    inputs = Input(shape=input_shape)\n",
        "    outputs = Average()([inception_gru_model(inputs), cnn_gru_dense_inception_time_model(inputs), inception_time_model(inputs)])\n",
        "\n",
        "    ensemble_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return ensemble_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5DPdqUqTav5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Average, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the ensemble model as the teacher (already defined in previous response)\n",
        "ensemble_model = ensemble_model(input_shape=(305, 1), num_classes=num_class)  # Adjust input shape and num_classes\n",
        "\n",
        "# Define the student model (your CNN_model)\n",
        "student_model = CNN_model(window_size=305)  # Assuming window_size is 305\n",
        "\n",
        "# Compile the student model (optional, for separate training if needed)\n",
        "student_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Function to compute the softened logits (teacher predictions)\n",
        "def temperature_softmax(predictions, temperature=4.0):\n",
        "    \"\"\"\n",
        "    Apply temperature scaling to predictions.\n",
        "    \"\"\"\n",
        "    return tf.nn.softmax(predictions / temperature)\n",
        "\n",
        "# Define the knowledge distillation loss\n",
        "def knowledge_distillation_loss(y_true, y_pred, teacher_predictions):\n",
        "    \"\"\"\n",
        "    Combine the cross-entropy loss with the distillation loss.\n",
        "    \"\"\"\n",
        "    alpha = 0.5  # Weight for the distillation loss (adjust as needed)\n",
        "    student_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true, y_pred)\n",
        "    teacher_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true, teacher_predictions)\n",
        "    soft_teacher_predictions = temperature_softmax(teacher_predictions)\n",
        "    distillation_loss = alpha * tf.keras.losses.kullback_leibler_divergence(soft_teacher_predictions, y_pred)\n",
        "    return student_loss + distillation_loss\n",
        "\n",
        "# Create a functional model for training the student with distillation\n",
        "inputs = Input(shape=(305, 1))\n",
        "student_predictions = student_model(inputs)\n",
        "teacher_predictions = ensemble_model(inputs)  # Pass input to the ensemble model\n",
        "loss = Lambda(knowledge_distillation_loss, arguments={'teacher_predictions': teacher_predictions})(student_predictions)\n",
        "distillation_model = Model(inputs=inputs, outputs=loss)\n",
        "\n",
        "# Compile the distillation model (using the custom loss function)\n",
        "distillation_model.compile(optimizer=\"adam\", loss=lambda y_true, y_pred: y_pred)  # Ignoring true labels\n",
        "\n",
        "# Train the student model using the distillation loss\n",
        "distillation_model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, X_val))\n",
        "\n",
        "# After training, the student_model should have improved performance due to knowledge transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVbs0SVrknba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have three models: model1, model2, model3\n",
        "\n",
        "# Create a VotingClassifier with 'soft' voting (weighted average of probabilities)\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('model', model),\n",
        "    ('model_base', model_base),\n",
        "    ('gru_model', gru_model),\n",
        "    ('CNN_GRU_Dense_model', CNN_GRU_Dense_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Fit the ensemble model on the training data\n",
        "ensemble_history=ensemble_model.fit(X_train, np.argmax(y_train, axis=1))  # Use argmax to get the class labels\n",
        "\n",
        "# Make predictions on the validation set\n",
        "ensemble_predictions = ensemble_model.predict(X_val)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "ensemble_accuracy = accuracy_score(np.argmax(y_val, axis=1), ensemble_predictions)\n",
        "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHByVjRJsb0g"
      },
      "outputs": [],
      "source": [
        "# Plot the training history\n",
        "plot_(ensemble_history)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "pre_cl(ensemble_model, X_val, y_val)\n",
        "plot_roc(ensemble_model,X_val,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjkeoGYdAxqH"
      },
      "outputs": [],
      "source": [
        "#how do i better the InceptionTime model created above? What can i do to optimize it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7kC4uNN7E-Y"
      },
      "outputs": [],
      "source": [
        "# prompt: auto download saved figures, assuming training is done on kaggle platforms\n",
        "\n",
        "!find . -name \"*.png\" -exec bash -c 'cat {} | xargs echo -n \"{}\" > /kaggle/working/files.txt' \\;\n",
        "!cat files.txt | while IFS= read -r line; do kaggle competitions submissions -c [YOUR_COMPETITION_NAME] -f \"$line\" -m \"My submission\"; done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPhEzEiBOsVZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FciwYrrDWEk"
      },
      "outputs": [],
      "source": [
        "nets = 5\n",
        "model = [0] *nets\n",
        "\n",
        "j=0\n",
        "#model[j] = Sequential()\n",
        "input_shape = Input(shape=(305, 1))\n",
        "#input_shape=(305,1)\n",
        "model[j]=layers.Conv1D(32,kernel_size=5,activation='relu')(input_shape)\n",
        "model[j]=layers.MaxPool1D()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=5,activation='relu')(model[j])\n",
        "model[j]=layers.GlobalMaxPool1D()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Reshape((1, 64))(model[j])\n",
        "model[j]=layers.GRU(128, activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Dense(10, activation='softmax')(model[j])\n",
        "model[j] = Model(inputs=input_shape, outputs=model[j])\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model[j].summary()\n",
        "\n",
        "j=1\n",
        "#model[j] = Sequential()\n",
        "model[j]=layers.Conv1D(32,kernel_size=3,activation='relu')(input_shape)\n",
        "model[j]=layers.Conv1D(32,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.MaxPool1D()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.GlobalMaxPool1D()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Reshape((1, 64))(model[j])\n",
        "model[j]=layers.GRU(128, activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Dense(10, activation='softmax')(model[j])\n",
        "model[j] = Model(inputs=input_shape, outputs=model[j])\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model[j].summary()\n",
        "\n",
        "j=2\n",
        "model[j]=layers.Conv1D(32,kernel_size=5,activation='relu')(input_shape)\n",
        "model[j]=layers.Conv1D(32,kernel_size=5,strides=2,padding='same',activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=5,activation='relu')(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=5,strides=2,padding='same',activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.GlobalMaxPool1D()(model[j])\n",
        "model[j]=layers.Reshape((1, 64))(model[j])\n",
        "model[j]=layers.GRU(128, activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Dense(10, activation='softmax')(model[j])\n",
        "model[j] = Model(inputs=input_shape, outputs=model[j])\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model[j].summary()\n",
        "\n",
        "j=3\n",
        "model[j]=layers.Conv1D(32,kernel_size=3,activation='relu')(input_shape)\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.Conv1D(32,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.Conv1D(32,kernel_size=5,strides=2,padding='same',activation='relu')(model[j])\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=3,activation='relu')(model[j])\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.Conv1D(64,kernel_size=5,strides=2,padding='same',activation='relu')(model[j])\n",
        "model[j]=layers.BatchNormalization()(model[j])\n",
        "model[j]=layers.GlobalMaxPool1D()(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Reshape((1, 64))(model[j])\n",
        "model[j]=layers.GRU(128, activation='relu')(model[j])\n",
        "model[j]=layers.Dropout(0.4)(model[j])\n",
        "model[j]=layers.Dense(10, activation='softmax')(model[j])\n",
        "model[j] = Model(inputs=input_shape, outputs=model[j])\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model[j].summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE1UQ4TmAsvQ"
      },
      "outputs": [],
      "source": [
        "model = InceptionTime(input_shape=input_shape, n_classes=n_classes, n_filters=128, n_modules=6)\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYijJwsnwQ1"
      },
      "outputs": [],
      "source": [
        "def check_com(model):\n",
        "\n",
        "    _params = model.count_params()\n",
        "    print(model.summary())\n",
        "\n",
        "    print(f\"Parameters: {_params}\")\n",
        "\n",
        "# Code cell <undefined>\n",
        "# # %% [code]\n",
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(3):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,padding='same',activation='relu',\n",
        "            input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    if j>0:\n",
        "        model[j].add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D())\n",
        "    if j>1:\n",
        "        model[j].add(Conv2D(96,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D(padding='same'))\n",
        "    model[j].add(Flatten())\n",
        "    #if j>0:\n",
        "\n",
        "    #model[j].add(Reshape((1, 64)))\n",
        "    #model[j].add(GRU(128, activation='relu'))\n",
        "\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNIt-wRsoSUl"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "names = [\"(C-P)x1\",\"(C-P)x2\",\"(C-P)x3\"]\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n",
        "# # TRAIN NETWORKS\n",
        "# history = [0] * nets\n",
        "#\n",
        "# epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmDCh-UHobae"
      },
      "outputs": [],
      "source": [
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu_M9aVTogXI"
      },
      "outputs": [],
      "source": [
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(3):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,padding='same',activation='relu',\n",
        "            input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    if j>0:\n",
        "        model[j].add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D())\n",
        "    if j>1:\n",
        "        model[j].add(Conv2D(96,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(GlobalMaxPool2D())\n",
        "    #model[j].add(Flatten())\n",
        "    if j==0:\n",
        "        model[j].add(Flatten())\n",
        "        model[j].add(Reshape((1, 6272)))\n",
        "    elif j==1:\n",
        "        model[j].add(Flatten())\n",
        "        model[j].add(Reshape((1, 3136)))\n",
        "    else:\n",
        "        model[j].add(Reshape((1, 96)))\n",
        "\n",
        "    model[j].add(GRU(128, activation='relu'))\n",
        "\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-RWiXv7MUXE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8x_K3g6pHnd"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"(C-P)x1\",\"(C-P)x2\",\"(C-P)x3\"]\n",
        "epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=32, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Train loss={2:.5f}, Validation accuracy={3:.5f}, Validation loss={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['loss']),\n",
        "#         max(history[j].history['val_accuracy']), max(history[j].history['val_loss']) ))\n",
        "\n",
        "\n",
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     #print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "# for i in range(nets):\n",
        "#     #print(max(history[i].history['val_loss']))\n",
        "#     plt.plot(history[i].history['val_loss'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LYU7R51p6s2"
      },
      "outputs": [],
      "source": [
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 6\n",
        "model = [0] *nets\n",
        "for j in range(6):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(j*8+8,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(j*16+16,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXpw9zQ-sgm8"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"8 maps\",\"16 maps\",\"24 maps\",\"32 maps\",\"48 maps\",\"64 maps\",\"128 maps\"]\n",
        "# epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=32, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n",
        "\n",
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3Hjp2nSsSDs"
      },
      "outputs": [],
      "source": [
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 6\n",
        "model = [0] *nets\n",
        "for j in range(6):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(j*8+8,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(j*16+16,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    if j == 0:\n",
        "        model[j].add(Reshape((1,256)))\n",
        "    elif j == 1:\n",
        "        model[j].add(Reshape((1,512)))\n",
        "    elif j == 2:\n",
        "        model[j].add(Reshape((1,768)))\n",
        "    elif j == 3:\n",
        "        model[j].add(Reshape((1,1024)))\n",
        "    elif j == 4:\n",
        "        model[j].add(Reshape((1,1280)))\n",
        "    elif j == 5:\n",
        "        model[j].add(Reshape((1,1536)))\n",
        "\n",
        "    model[j].add(GRU(256, activation='relu'))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS2PRC-TuWJw"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"8 maps\",\"16 maps\",\"24 maps\",\"32 maps\",\"48 maps\",\"64 maps\",\"128 maps\"]\n",
        "# epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=32, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n",
        "\n",
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCuoMJtkzvUU"
      },
      "outputs": [],
      "source": [
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(128, activation='relu'))\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCRgN3Nxv2QI"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"0N\",\"32N\",\"64N\",\"128N\",\"256N\",\"512N\",\"1024N\",\"2048N\"]\n",
        "# epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=80, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n",
        "\n",
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DSeuSX7vZ4b"
      },
      "outputs": [],
      "source": [
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(GlobalMaxPool2D())\n",
        "\n",
        "    if j>0:\n",
        "      model[j].add(Reshape((1,64)))\n",
        "      model[j].add(GRU(2**(j+4), activation='relu'))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCU5ZUKSz9jd"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"0N\",\"32N\",\"64N\",\"128N\",\"256N\",\"512N\",\"1024N\",\"2048N\"]\n",
        "# epochs = 20\n",
        "# for j in range(nets):\n",
        "#     history[j] = model[j].fit(X_train2,Y_train2, batch_size=80, epochs = epochs,\n",
        "#         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "#     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "#         names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n",
        "\n",
        "# # PLOT ACCURACIES\n",
        "# plt.figure(figsize=(15,5))\n",
        "# for i in range(nets):\n",
        "#     print(max(history[i].history['val_accuracy']))\n",
        "#     plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(names, loc='upper left')\n",
        "# axes = plt.gca()\n",
        "# #axes.set_ylim([0.98,1])\n",
        "# plt.show()\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ674stkzjN6"
      },
      "outputs": [],
      "source": [
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(128, activation='relu'))\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7mDJSvZY4eT"
      },
      "outputs": [],
      "source": [
        " #CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"D=0\",\"D=0.1\",\"D=0.2\",\"D=0.3\",\"D=0.4\",\"D=0.5\",\"D=0.6\",\"D=0.7\"]\n",
        "epochs = 30\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmGHfy5SyHR6"
      },
      "outputs": [],
      "source": [
        "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(GlobalMaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Reshape((1,64)))\n",
        "    model[j].add(GRU(128, activation='relu'))\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Dense(9, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_pvabveZCuv"
      },
      "outputs": [],
      "source": [
        "#CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n",
        "# TRAIN NETWORKS\n",
        "history = [0] * nets\n",
        "names = [\"D=0\",\"D=0.1\",\"D=0.2\",\"D=0.3\",\"D=0.4\",\"D=0.5\",\"D=0.6\",\"D=0.7\"]\n",
        "epochs = 30\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpYEhGfCZ0eR"
      },
      "outputs": [],
      "source": [
        "nets = 5\n",
        "model = [0] *nets\n",
        "\n",
        "j=0\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=1\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=2\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=3\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=4\n",
        "model[j] = Sequential()\n",
        "\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(128, activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jxQ8X0KaHap"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.2)\n",
        "# TRAIN NETWORKS 1,2,3,4\n",
        "history = [0] * nets\n",
        "names = [\"basic\",\"32C3-32C3\",\"32C5S2\",\"both+BN\",\"both+BN+DA\"]\n",
        "epochs = 100\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTwIuDOwaTTD"
      },
      "outputs": [],
      "source": [
        "nets = 5\n",
        "model = [0] *nets\n",
        "\n",
        "j=0\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Reshape((1,1024)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=1\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Reshape((1,1024)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=2\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Reshape((1,1024)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=3\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Reshape((1,1024)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=4\n",
        "model[j] = Sequential()\n",
        "\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Reshape((1,1024)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7krLKTuaWTo"
      },
      "outputs": [],
      "source": [
        "# CREATE VALIDATION SET\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.2)\n",
        "# TRAIN NETWORKS 1,2,3,4\n",
        "history = [0] * nets\n",
        "names = [\"basic\",\"32C3-32C3\",\"32C5S2\",\"both+BN\",\"both+BN+DA\"]\n",
        "epochs = 100\n",
        "run_and_plot(names, annealer, nets, styles, X_train2, X_val2, Y_train2, Y_val2 , model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9yVuQOLau_w"
      },
      "outputs": [],
      "source": [
        "nets = 5\n",
        "model = [0] *nets\n",
        "\n",
        "j=0\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(GlobalMaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Reshape((1,64)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=1\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(MaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(globalMaxPool2D())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Reshape((1,64)))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=2\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(globalMaxPool2D())\n",
        "model[j].add(Reshape((1,64)))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=3\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(globalMaxPool2D())\n",
        "model[j].add(Reshape((1,64)))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n",
        "\n",
        "j=4\n",
        "model[j] = Sequential()\n",
        "\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(globalMaxPool2D())\n",
        "model[j].add(Reshape((1,64)))\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(GRU(128, activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "check_com(model[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV_UjB5jHUWh"
      },
      "outputs": [],
      "source": [
        "# prompt: write a code to go through this kaggle directory \" kaggle /kaggle/working/ \" of this website \"https://www.kaggle.com/code/maminu0/notebook4b85fc2fb0/edit\" and dowload all available files to my desktop\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def download_kaggle_files(url, directory):\n",
        "  # Send an HTTP GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Parse the HTML content of the page\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # Find all links to files within the specified directory\n",
        "  file_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].startswith(directory)]\n",
        "\n",
        "  # Download each file\n",
        "  for file_link in file_links:\n",
        "    file_url = 'https://www.kaggle.com' + file_link\n",
        "    file_name = os.path.basename(file_link)\n",
        "    file_path = os.path.join('/content', file_name)  # Assuming you want to download to Colab's /content directory\n",
        "\n",
        "    print(f\"Downloading {file_name}...\")\n",
        "    response = requests.get(file_url)\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "      f.write(response.content)\n",
        "\n",
        "    print(f\"Downloaded {file_name} to {file_path}\")\n",
        "\n",
        "# Example usage\n",
        "kaggle_url = 'https://www.kaggle.com/code/maminu0/notebook4b85fc2fb0/edit'\n",
        "kaggle_directory = 'kaggle/working/'\n",
        "\n",
        "download_kaggle_files(kaggle_url, kaggle_directory)\n",
        "\n",
        "# After downloading to Colab, you can use the following to download to your local machine\n",
        "from google.colab import files\n",
        "for file_name in os.listdir('/content'):\n",
        "  if file_name.startswith('kaggle_working_'):\n",
        "    files.download(os.path.join('/content', file_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sJMSAkZIfgp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlApiemC7iU9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def remove_folder_contents(folder):\n",
        "    for the_file in os.listdir(folder):\n",
        "      if the_file.endswith(\".png\"):\n",
        "          file_path = os.path.join(folder, the_file)\n",
        "          try:\n",
        "              if os.path.isfile(file_path):\n",
        "                  os.unlink(file_path)\n",
        "              elif os.path.isdir(file_path):\n",
        "                  remove_folder_contents(file_path)\n",
        "                  os.rmdir(file_path)\n",
        "          except Exception as e:\n",
        "              print(e)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "folder_path = '/kaggle/working'\n",
        "remove_folder_contents(folder_path)\n",
        "os.rmdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwlUlFejFdNu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, GlobalMaxPool2D, Reshape, GRU, Dense, Attention\n",
        "\n",
        "j = 0\n",
        "model[j] = Sequential()\n",
        "\n",
        "model[j].add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(64, 64, 1)))\n",
        "model[j].add(Dropout(0.1))\n",
        "model[j].add(MaxPool2D(2))\n",
        "\n",
        "model[j].add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "model[j].add(Dropout(0.1))\n",
        "model[j].add(MaxPool2D(2))\n",
        "\n",
        "model[j].add(Conv2D(96, kernel_size=3, activation='relu'))\n",
        "model[j].add(Conv2D(96, kernel_size=3, activation='relu'))\n",
        "model[j].add(Dropout(0.3))\n",
        "\n",
        "model[j].add(GlobalMaxPool2D())\n",
        "model[j].add(Reshape((1, 96)))\n",
        "\n",
        "# Replace the Bidirectional GRU with a GRU followed by an Attention layer\n",
        "model[j].add(GRU(256, activation='relu', return_sequences=True))\n",
        "\n",
        "# Apply self-attention by passing the same tensor as query and value\n",
        "def apply_attention(inputs):\n",
        "    # inputs is a tensor with shape (batch_size, timesteps, features)\n",
        "    # We apply self-attention, so query and value are the same\n",
        "    query = value = inputs\n",
        "    key = None  # For self-attention, key can be omitted\n",
        "    attention_output = Attention()([query, value, key])\n",
        "    return attention_output\n",
        "\n",
        "# Use a Lambda layer to apply the attention function\n",
        "from tensorflow.keras.layers import Lambda\n",
        "model[j].add(Lambda(apply_attention))\n",
        "\n",
        "# model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.1))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvkWaIeWJMZW"
      },
      "outputs": [],
      "source": [
        "# prompt: Modify this code to include attention mechanism\n",
        "# j=6\n",
        "# model[j] = Sequential()\n",
        "# model[j].add(Conv2D(64,kernel_size=3,activation='relu',input_shape=(64,64,1)))\n",
        "# model[j].add(Dropout(0.1))\n",
        "# # model[j].add(BatchNormalization())\n",
        "# model[j].add(MaxPool2D(2))\n",
        "# model[j].add(Conv2D(96,kernel_size=3,activation='relu'))\n",
        "# model[j].add(Dropout(0.1))\n",
        "# # model[j].add(BatchNormalization())\n",
        "# model[j].add(MaxPool2D(2))\n",
        "\n",
        "j=0\n",
        "model[j] = Sequential()\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu',input_shape=(64,64,1)))\n",
        "model[j].add(Dropout(0.1))\n",
        "# model[j].add(BatchNormalization())\n",
        "model[j].add(MaxPool2D(2))\n",
        "model[j].add(Conv2D(96,kernel_size=3,activation='relu'))\n",
        "model[j].add(Dropout(0.1))\n",
        "# model[j].add(BatchNormalization())\n",
        "model[j].add(MaxPool2D(2))\n",
        "model[j].add(Conv2D(128, kernel_size=3, activation='relu'))\n",
        "model[j].add(Conv2D(128, kernel_size=3, activation='relu'))\n",
        "model[j].add(Dropout(0.3))\n",
        "model[j].add(GlobalMaxPool2D())\n",
        "model[j].add(Reshape((1, 128)))\n",
        "model[j].add(GRU(256, activation='relu', return_sequences=True))\n",
        "# Apply self-attention by passing the same tensor as query and value\n",
        "def apply_attention(inputs):\n",
        "    # inputs is a tensor with shape (batch_size, timesteps, features)\n",
        "    # We apply self-attention, so query and value are the same\n",
        "    query = value = inputs = key\n",
        "    key = None  # For self-attention, key can be omitted\n",
        "    attention_output = Attention()([query, value, key])\n",
        "    return attention_output\n",
        "# Use a Lambda layer to apply the attention function\n",
        "model[j].add(Lambda(apply_attention))\n",
        "# model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.1))\n",
        "model[j].add(Dense(9, activation='softmax'))\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltgIPKH7J1tl"
      },
      "outputs": [],
      "source": [
        "model[j].summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAoFcDzrMkvZ"
      },
      "outputs": [],
      "source": [
        "def plot_(hist, name=\"finals_pattern_plot\"):\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = hist.history['accuracy']\n",
        "    train_loss = hist.history['loss']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    fig.set_size_inches(16,9)\n",
        "\n",
        "    ax[0].plot( train_acc , 'go-' , label = 'Training Accuracy')\n",
        "    ax[0].plot( val_acc , 'ro-' , label = 'Testing Accuracy')\n",
        "    ax[0].set_title('Training & Validation Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "    ax[1].plot( train_loss , 'g-o' , label = 'Training Loss')\n",
        "    ax[1].plot( val_loss , 'r-o' , label = 'Testing Loss')\n",
        "    ax[1].set_title('Testing Accuracy & Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "    plt.savefig(f\"{'_'.join(name)}_{str(time.time())}_{name}_.png\")\n",
        "\n",
        "def pre_cl(model,X_val2,Y_val2, name=\"finals\"):\n",
        "    predictions = model.predict(X_val2)\n",
        "    Xpred = np.argmax(predictions, axis=1)\n",
        "    ytrue = np.argmax(Y_val2, axis=1)\n",
        "    classes = [\"Class \" + str(i) for i in range(9) if i != 9]\n",
        "    print(classification_report(ytrue, Xpred))#, target_names = classes)\n",
        "    correct = np.nonzero(Xpred == ytrue)[0]\n",
        "    plt.figure(figsize = (15,15))\n",
        "    i = 0\n",
        "    for c in correct[:6]:\n",
        "        plt.subplot(3,2,i+1)\n",
        "        plt.imshow(X_val2[c], cmap=\"gray\", interpolation='none')\n",
        "        plt.title(\"Predicted Class {},Actual Class {}\".format(Xpred[c], ytrue[c]))\n",
        "        plt.tight_layout()\n",
        "        i += 1\n",
        "    plt.savefig(f\"{'_'.join(name)}_{str(time.time())}_{name}_.png\")\n",
        "    cm = confusion_matrix(ytrue,Xpred)\n",
        "    sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-gxIN0UK8sN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnX1cbEPLhNU"
      },
      "outputs": [],
      "source": [
        "plot_(hist6, name=names[6])\n",
        "pre_cl(model[6],X_val2,Y_val2, name=names[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2P0Ny1VQ1WR"
      },
      "outputs": [],
      "source": [
        "Modify the code below to use multi-head attention from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGYmVqp2Q20Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Input, Dense, Flatten, Conv2D, Dropout, MaxPool2D, GlobalMaxPool2D, Reshape, GRU, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "j=0\n",
        "# Use the Functional API to create a model with multiple inputs\n",
        "input_tensor = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(32,kernel_size=3,activation='relu')(input_tensor)\n",
        "x = Dropout(0.1)(x)\n",
        "x = MaxPool2D(2)(x)\n",
        "x = Conv2D(64,kernel_size=3,activation='relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = MaxPool2D(2)(x)\n",
        "x = Conv2D(96, kernel_size=3, activation='relu')(x)\n",
        "x = Conv2D(96, kernel_size=3, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = GlobalMaxPool2D()(x)\n",
        "x = Reshape((1, 96))(x)\n",
        "x = LSTM(256, activation='relu', return_sequences=True)(x)\n",
        "\n",
        "# Use the same tensor as both 'query' and 'value' for self-attention\n",
        "attention_output = MultiHeadAttention(num_heads=8, key_dim=128)(x, x)\n",
        "\n",
        "x = Dropout(0.1)(attention_output)\n",
        "x = Flatten()(x)\n",
        "output_tensor = Dense(9, activation='softmax')(x)\n",
        "\n",
        "model[j] = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model[j].summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-bOPAGR4Q1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.3, stratify=Y_train, random_state=42)\n",
        "#hist6 = model[0].fit(X_train2,Y_train2, epochs = 200, batch_size=128, verbose=0, validation_data=(X_val2,Y_val2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnDeKzDNTLEa"
      },
      "outputs": [],
      "source": [
        "X_train2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYUKzYpfSJgL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "hist6 = model[0].fit(X_train2,Y_train2, epochs = 200, batch_size=128, verbose=0, validation_data=(X_val2,Y_val2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tencf_dRTlWo"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plot_(hist6)#, name=names[0])\n",
        "pre_cl(model[0],X_val2,Y_val2)#, name=names[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwxs6T1QkszG"
      },
      "outputs": [],
      "source": [
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train_, Y_train_, test_size = 0.333)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3DZPZ2slnOB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dropout, MaxPool2D, GlobalMaxPool2D, Reshape, Bidirectional, LSTM, Dense, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Hyperparameters search space\n",
        "units = [8, 16, 32, 64]#, 96, 128]\n",
        "kernel_sizes = [2,3]#, 5]#, 7]\n",
        "activation = ['relu', 'elu', 'selu']\n",
        "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "maxpool = [2, 3]\n",
        "\n",
        "# Example Dataset (For simplicity, using random data)\n",
        "#X = np.random.rand(1000, 28, 28, 1)\n",
        "#y = to_categorical(np.random.randint(9, size=1000))\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = X_train2, X_val2, Y_train2, Y_val2#train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ze-HYg8lbXw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the ABC Algorithm\n",
        "class ABCAlgorithm:\n",
        "    def __init__(self, num_bees, max_iterations, dataset, labels, limit=10):\n",
        "        self.num_bees = num_bees\n",
        "        self.max_iterations = max_iterations\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "        self.limit = limit\n",
        "        # Initialize random solutions (hyperparameter combinations)\n",
        "        self.population = [self.random_solution() for _ in range(self.num_bees)]\n",
        "        self.fitness = np.zeros(self.num_bees)\n",
        "        self.trial_counter = np.zeros(self.num_bees)\n",
        "\n",
        "    def random_solution(self):\n",
        "        return {\n",
        "            'units': np.random.choice(units),\n",
        "            'kernel_size': np.random.choice(kernel_sizes),\n",
        "            'activation': np.random.choice(activation),\n",
        "            'dropout': np.random.choice(dropout),\n",
        "            'maxpool': np.random.choice(maxpool)\n",
        "        }\n",
        "\n",
        "    def evaluate_fitness(self):\n",
        "        for i in range(self.num_bees):\n",
        "            self.fitness[i] = self.evaluate_single_fitness(self.population[i])\n",
        "\n",
        "    def employed_bees_phase(self):\n",
        "        for i in range(self.num_bees):\n",
        "            new_solution = self.mutate_solution(self.population[i])\n",
        "            new_fitness = self.evaluate_single_fitness(new_solution)\n",
        "            if new_fitness > self.fitness[i]:\n",
        "                self.population[i] = new_solution\n",
        "                self.fitness[i] = new_fitness\n",
        "                self.trial_counter[i] = 0\n",
        "            else:\n",
        "                self.trial_counter[i] += 1\n",
        "\n",
        "    def onlooker_bees_phase(self):\n",
        "        fitness_prob = self.fitness / self.fitness.sum()\n",
        "        for i in range(self.num_bees):\n",
        "            selected_bee = np.random.choice(range(self.num_bees), p=fitness_prob)\n",
        "            new_solution = self.mutate_solution(self.population[selected_bee])\n",
        "            new_fitness = self.evaluate_single_fitness(new_solution)\n",
        "            if new_fitness > self.fitness[selected_bee]:\n",
        "                self.population[selected_bee] = new_solution\n",
        "                self.fitness[selected_bee] = new_fitness\n",
        "                self.trial_counter[selected_bee] = 0\n",
        "            else:\n",
        "                self.trial_counter[selected_bee] += 1\n",
        "\n",
        "    def scout_bees_phase(self):\n",
        "        for i in range(self.num_bees):\n",
        "            if self.trial_counter[i] > self.limit:\n",
        "                self.population[i] = self.random_solution()\n",
        "                self.fitness[i] = self.evaluate_single_fitness(self.population[i])\n",
        "                self.trial_counter[i] = 0\n",
        "\n",
        "    def mutate_solution(self, solution):\n",
        "        new_solution = solution.copy()\n",
        "        param_to_mutate = np.random.choice(list(solution.keys()))\n",
        "        if param_to_mutate == 'units':\n",
        "            new_solution['units'] = np.random.choice(units)\n",
        "        elif param_to_mutate == 'kernel_size':\n",
        "            new_solution['kernel_size'] = np.random.choice(kernel_sizes)\n",
        "        elif param_to_mutate == 'activation':\n",
        "            new_solution['activation'] = np.random.choice(activation)\n",
        "        elif param_to_mutate == 'dropout':\n",
        "            new_solution['dropout'] = np.random.choice(dropout)\n",
        "        elif param_to_mutate == 'maxpool':\n",
        "            new_solution['maxpool'] = np.random.choice(maxpool)\n",
        "        return new_solution\n",
        "\n",
        "    def build_and_compile_model(self, hyperparams):\n",
        "\n",
        "        kernel_size_tuple = (hyperparams['kernel_size'], hyperparams['kernel_size'])  # Convert kernel_size to a tuple\n",
        "        pool_size_tuple = (hyperparams['maxpool'], hyperparams['maxpool'])  # Convert kernel_size to a tuple\n",
        "        input_tensor = Input(shape=(64, 64, 1))\n",
        "        x = Conv2D(hyperparams['units'], kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(input_tensor)\n",
        "        x = Dropout(hyperparams['dropout'])(x)\n",
        "        x = MaxPool2D(pool_size_tuple)(x)\n",
        "        x = Conv2D(hyperparams['units']+32, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "        x = Dropout(hyperparams['dropout'])(x)\n",
        "        x = MaxPool2D(pool_size_tuple)(x)\n",
        "        x = Conv2D(hyperparams['units']+64, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "        x = Conv2D(hyperparams['units']+64, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "        x = Dropout(hyperparams['dropout'])(x)\n",
        "        x = GlobalMaxPool2D()(x)\n",
        "        x = Reshape((1, hyperparams['units']+64))(x)\n",
        "        x = Bidirectional(LSTM(hyperparams['units']+64))(x)\n",
        "        x = Dropout(hyperparams['dropout'])(x)\n",
        "        x = Flatten()(x)\n",
        "        output_tensor = Dense(9, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def evaluate_single_fitness(self, hyperparams):\n",
        "        model = self.build_and_compile_model(hyperparams)\n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=0)  # Use smaller epochs for faster evaluation\n",
        "        loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        return accuracy\n",
        "\n",
        "    def optimize(self):\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.evaluate_fitness()\n",
        "            self.employed_bees_phase()\n",
        "            self.onlooker_bees_phase()\n",
        "            self.scout_bees_phase()\n",
        "            best_bee_idx = np.argmax(self.fitness)\n",
        "            best_solution = self.population[best_bee_idx]\n",
        "            print(f\"Iteration {iteration}: Best Fitness = {self.fitness[best_bee_idx]}\")\n",
        "        return best_solution\n",
        "\n",
        "# Define the GTO Algorithm\n",
        "class GTOAlgorithm:\n",
        "    def __init__(self, solution, dataset, labels):\n",
        "        self.solution = solution\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "\n",
        "    def refine(self):\n",
        "        selected_solution = self.solution\n",
        "        model = self.build_and_compile_model(selected_solution)\n",
        "\n",
        "        model.fit(X_train, y_train, epochs=30, batch_size=128, verbose=0)  # Fine-tuning with more epochs\n",
        "        loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        print(f\"Refined Solution Accuracy: {accuracy}\")\n",
        "        return selected_solution\n",
        "\n",
        "\n",
        "    def build_and_compile_model(self, hyperparams):\n",
        "      pool_size_tuple = (hyperparams['maxpool'], hyperparams['maxpool'])  # Convert kernel_size to a tuple\n",
        "      kernel_size_tuple = (hyperparams['kernel_size'], hyperparams['kernel_size'])  # Convert kernel_size to a tuple\n",
        "      input_tensor = Input(shape=(64, 64, 1))\n",
        "      x = Conv2D(hyperparams['units'], kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(input_tensor)\n",
        "      x = Dropout(hyperparams['dropout'])(x)\n",
        "      x = MaxPool2D(pool_size_tuple)(x)\n",
        "      x = Conv2D(hyperparams['units']+32, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "      x = Dropout(hyperparams['dropout'])(x)\n",
        "      x = MaxPool2D(pool_size_tuple)(x)\n",
        "      x = Conv2D(hyperparams['units']+64, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "      x = Conv2D(hyperparams['units']+64, kernel_size=kernel_size_tuple, activation=hyperparams['activation'])(x)\n",
        "      x = Dropout(hyperparams['dropout'])(x)\n",
        "      x = GlobalMaxPool2D()(x)\n",
        "      x = Reshape((1, hyperparams['units']+64))(x)\n",
        "      x = Bidirectional(LSTM(hyperparams['units']+64))(x)\n",
        "      x = Dropout(hyperparams['dropout'])(x)\n",
        "      x = Flatten()(x)\n",
        "      output_tensor = Dense(9, activation='softmax')(x)\n",
        "\n",
        "      model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "      model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "      return model\n",
        "\n",
        "\n",
        "# Combine ABC and GTO\n",
        "def hybrid_abc_gto(dataset, labels, num_bees=10, max_iterations=5):\n",
        "    abc = ABCAlgorithm(num_bees, max_iterations, dataset, labels)\n",
        "    best_solution = abc.optimize()\n",
        "\n",
        "    gto = GTOAlgorithm(best_solution, dataset, labels)\n",
        "    refined_solution = gto.refine()\n",
        "\n",
        "    return refined_solution\n",
        "\n",
        "# Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJrCmoOhl7Y-"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters search space\n",
        "units = [16, 32, 64]#, 96, 128]\n",
        "kernel_sizes = [2,3]#, 5]#, 7]\n",
        "activation = ['relu', 'elu', 'selu']\n",
        "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "maxpool = [2, 3]\n",
        "best_hyperparameters = hybrid_abc_gto(X_train_, Y_train_)\n",
        "print(\"Best Hyperparameters found:\", best_hyperparameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUvVNFkh4_5y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulU--RsN5Ak3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyoJVXvG5BBA"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters search space\n",
        "units = [8, 16, 32, 64]#, 96, 128]\n",
        "kernel_sizes = [2,3]#, 5]#, 7]\n",
        "activation = ['relu', 'elu', 'selu']\n",
        "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "maxpool = [2, 3]\n",
        "\n",
        "best_hyperparameters = hybrid_abc_gto(X_train_, Y_train_)\n",
        "print(\"Best Hyperparameters found:\", best_hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Of-bs06eD4"
      },
      "outputs": [],
      "source": [
        "Best Hyperparameters found: {'units': 32, 'kernel_size': 3, 'activation': 'relu', 'dropout': 0.1, 'maxpool': 2}\n",
        "Best Hyperparameters found: {'units': 32, 'kernel_size': 3, 'activation': 'relu', 'dropout': 0.2, 'maxpool': 2}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}